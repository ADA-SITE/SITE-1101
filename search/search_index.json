{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udf93 SITE 1101: Principles of Information Systems","text":"<p>The course provides a general overview of what SITE students of the ADA University will study throughout their bachelor\u2019s program. It introduces the fundamentals of information technology and computer-based systems, including hardware, software, network, database, artificial intelligence, system analysis and design, programming, etc. The course was originally developed by Araz Yusubov and Nuraddin Sadili with iterations over the years.</p>"},{"location":"#fall-2025","title":"Fall 2025","text":"<p>This website was launched at the end of the Fall 2025 semester with the efforts of our great Teaching Assistants Nilufar Ismayilova, Rahida Asadli (see Lecture Notes), and Rahman Karimov (Website), as well as the guidance of the course instructors Rumiyya Alili and Ismayil Shahaliyev.</p>"},{"location":"#spring-2026","title":"Spring 2026","text":"<p>The course was taught by Ismayil Shahaliyev and Araz Yusubov. The website was migrated to <code>mkdocs</code> with updates for the ongoing semester. Project 4 was brought back with certain revision of the course content.</p>"},{"location":"#office-hours","title":"Office Hours","text":"<p>Students are required to consistently check the course Blackboard page for announcements. General-purpose questions useful to other students should be asked on Blackboard discussion page. For private matters, students should use email. See communication rules for further details. TA office hours for the semester will be shared on blackboard.</p>"},{"location":"#technological-requirements","title":"Technological Requirements","text":"<p>Students are to use their laptops to implement the class assignments. Github account, downloaded git version control system, and IDE (VSCode or Cursor) will also be needed. Additional requirements may be provided by the instructor during the semester.</p>"},{"location":"#deadline-policy","title":"Deadline Policy","text":"<p>Deadlines will never be extended. When submitting, a student must consider possible internet connection issues, uploaded file size, etc. Late submissions of one minute until thirty minutes will receive a 10% penalty, late submissions of thirty minutes until one day will receive a 25% penalty with no claim for bonuses (if there are any), late submissions of more than one day will receive zero.</p>"},{"location":"#participation","title":"Participation","text":"<p>Students are expected to review the material covered in class and come prepared for the next session\u2019s discussions. Participation will be encouraged through both general discussion questions and individual questions directed to selected students.<sup>1</sup> For excused absences (e.g. illnesses), a student must write an email with a document of proof to their advisor, who will inform the relevant instructors.</p>"},{"location":"#homeworks-and-projects","title":"Homeworks and Projects","text":"<p>The detailed instructions and grading criteria of homeworks and projects will be communicated to students during the semester. Students are expected follow the instructions and submit their assignments before the deadline. </p>"},{"location":"#examination","title":"Examination","text":"<p>You will have two exams: midterm and final. Both exams will be computer-based (unless noted otherwise) with questions of various types (e.g. multiple-choice, fill-in-the-blank, code/essay). More specific rules will be communicated to you before the exam.<sup>2</sup></p>"},{"location":"#study-materials","title":"Study Materials","text":"<p>Study materials will be shared with you on blackboard throughout the semester. Lecture Notes are available online for this course.</p> <ol> <li> <p>In case of difficulties in communication, you are encouraged to consult your academic advisor and seek psychological support.\u00a0\u21a9</p> </li> <li> <p>You are expected to be aware of the Exam Rules and Regulations. If you have special needs or health  issues, you are strongly recommended to contact the University\u2019s Student Academic Support Services well ahead of the examination date. It is your responsibility to manage conflicts in your schedule and notify your instructors about it at least two weeks before the examination date.\u00a0\u21a9</p> </li> </ol>"},{"location":"Lecture_Notes/01_intro/","title":"01. Introduction","text":"<p> Rahida Asadli, Ismayil Shahaliyev Oct 10, 2025 Jan 30, 2026</p> <p>Engineering (from Latin ingenium: cleverness) is the creative application of scientific principles to design, build, operate, and predict the behavior of structures, machines, processes, or systems in a safe and efficient manner to achieve a specific objective.</p> <p>Technology (from ancient Greek \u03c4\u03ad\u03c7\u03bd\u03b7 + \u03bb\u03bf\u03b3\u03af\u03b1 = techne + logia: science + art, skill) is the application of techniques, skills, methods, and processes to produce goods and services or achieve specific objectives.</p>"},{"location":"Lecture_Notes/01_intro/#data-information-knowledge","title":"Data, Information, Knowledge","text":"<p>Data (plural, from Latin datum - something given) are raw facts, figures, or symbols without context or interpretation. They represent unprocessed observations and have little meaning on their own. Information is data that has been processed, organized, or structured in a way that gives it context and meaning. Knowledge is the understanding, insight, and experience gained from interpreting and applying information.</p> <p>Note</p> <p>Data: \"30,\" \"32,\" \"31\" just raw numbers with no context. Information: \"The temperatures recorded over three days were 30\u00b0C, 32\u00b0C, and 31\u00b0C.\" - now the data has context and meaning. Knowledge: \"Since temperatures above 30\u00b0C increase water evaporation, watering plants in the early morning is more effective.\" - applying the information to guide action.</p> <p>Context is the background, circumstances, or setting that gives meaning to data, information, or events. It allows us to interpret and understand facts correctly by showing how they relate to one another and to the situation in which they occur.</p> <p>Note</p> <p>Numbers like 1, 4, 9, 16, and 25 by themselves are just data. Without context, our experience may lead us to assume they are the squares of numbers from 1 to 5. But such a quick pattern recognition may mislead us once the context is given that these numbers represent the quantity of ice cream sold in a small grocery store on five different days. Context gives data its true significance and turns it into meaningful information.</p>"},{"location":"Lecture_Notes/01_intro/#system","title":"System","text":"<p>Cybernetics is the study of control, communication, and feedback in systems, a field developed by Norbert Wiener. During WWII, Wiener worked on predicting the future position of enemy aircraft: he realized that the gun\u2013pilot system forms a feedback system, constantly correcting actions based on error and delay rather than following a fixed plan. This insight led him to extend control theory beyond machines to living systems, emphasizing self-regulation and adaptation under uncertainty. This way of thinking later influenced artificial intelligence and deep learning, where feedback from errors is used to adjust internal parameters and improve performance.</p> <p>Tip</p> <p>We will discuss artificial intelligence in our future weeks. See the lecture notes dedicated to the topic.</p> <p>System is a set of interrelated components working together toward a common goal by accepting inputs, processing them, and producing outputs. Control system is a system that continuously monitors outputs and uses feedback to adjust inputs or processes, maintaining the system's desired performance and stability.</p>      By Intgr \u2013     Own work,     Public Domain,     Link <p>Note</p> <p>In a home heating system, the desired room temperature is the input, the furnace heating the air is the process, and the warm air circulating through the house is the output. Feedback comes from a thermostat, which senses the actual temperature and switches the furnace on or off to keep the room at the set temperature.</p> <ul> <li>Input. The resources (data, materials, or energy) that enter the system.</li> <li>Process. The transformation mechanism that converts input into output.</li> <li>Output. The result produced by the system.</li> <li>Feedback. Information from the output that is sent back to the system to influence future inputs or the process itself.</li> </ul> <p>Note</p> <p>In the mid-19th century, several suspension bridges across Europe failed due to poor system design - weak materials and unpredictable environmental conditions. When John Roebling proposed the Brooklyn Bridge, he applied engineering systems thinking to overcome these failures. Roebling's bridge demonstrates how system components and interactions determine reliability and success. When one subsystem (e.g., faulty cables) failed, built-in redundancies maintained the system's stability.</p> <p>Information system is a set of interrelated components that collect, process, store, disseminate data and information, and provide a feedback mechanism to monitor and control its operation to make sure it continues to meet its goals and objectives.</p> <p>Note</p> <p>A library information system records books (input), organizes and catalogs them (process), provides search results for users (output), and updates records based on borrowing activity (feedback).</p> <p>Computer-Based Information System is a single set of</p> <ul> <li>hardware,</li> <li>software,</li> <li>databases,</li> <li>telecommunications,</li> <li>people, and</li> <li>procedures</li> </ul> <p>that are configured to collect, manipulate, store, and process data into information.</p> <p>Note</p> <p>An online banking system takes transaction requests as input, processes them through secure software, and provides account balances or confirmations as output. Feedback such as error alerts or user actions improves future transactions and system reliability.</p>"},{"location":"Lecture_Notes/01_intro/#additional-material","title":"Additional Material","text":"<ul> <li>One of the most epic engineering feats in history</li> <li>How do Steam Engines Work?</li> <li>The Meaning of Knowledge</li> </ul>"},{"location":"Lecture_Notes/02_binary/","title":"02. Binary Representation, Arithmetic &amp; Logic Operations","text":"<p> Nilufar Ismayilova, Ismayil Shahaliyev Oct 23, 2025 Jan 30, 2026</p>"},{"location":"Lecture_Notes/02_binary/#digital-vs-analog","title":"Digital vs Analog","text":"<p>Discrete means values change in separate, well-defined steps with no possible values in between. Continuous means values can vary smoothly and without breaks over a range \u2014 between any two values, there are infinitely many possible intermediate values.</p> <p>Note</p> <p>A standard light switch is either on or off \u2014 there is nothing between those two states \u2014 hence, it is discrete. Likewise, a digital clock that shows <code>14:35</code> jumps straight to <code>14:36</code> with no \"in-between\" time displayed. In a mercury thermometer, however, if the temperature is \\(21\u00b0C\\) and then rises to \\(22\u00b0C\\), it passes through \\(21.1\u00b0C\\), \\(21.11\u00b0C\\), \\(21.111\u00b0C\\), and so on. The temperature does not \"jump\" from one reading to another. It changes continuously.</p> <p>This distinction is why digital systems are far more noise-tolerant than analog ones. In a continuous analog system, even a tiny fluctuation (temperature drift or electrical noise) can change the value. In a discrete digital system, as long as the signal is close enough to one of the two states (for example, above \\(3V\\) is interpreted as \\(1\\), below \\(1V\\) as \\(0\\), with an undefined region in between), the system will interpret it correctly. This tolerance to noise is why modern computers rely almost entirely on discrete binary states.</p> <p>Note</p> <p>In an analog system, numbers \\(3\\) and \\(5\\) could be represented by physical quantities such as voltage levels of \\(3V\\) and \\(5V\\). To add them, the machine might combine the voltages to produce \\(8V\\). But if noise shifts levels by just \\(0.2V\\), the output might become \\(7.8V\\) or \\(8.2V\\), which no longer corresponds exactly to \\(8\\). Repeated operations would accumulate these errors, making the result unreliable.</p> <p>Digital circuits only need to distinguish between a small set of clearly separated values, which makes them more reliable, scalable, and resistant to noise. The simplest and most robust choice for that was the usage two discrete states. This directly matches what electronic components can reliably detect: current flowing or not, voltage high or low, transistor open or closed. Those two states are naturally represented by \\(0\\) and \\(1\\), forming the binary number system.</p>"},{"location":"Lecture_Notes/02_binary/#bit-byte-data-units","title":"Bit, Byte, Data Units","text":"<p>All digital information \u2014 including instructions themselves \u2014 is expressed as bits (binary digits, formally introduced by Claude Shannon in A Mathematical Theory of Communication). A bit is the smallest unit of data and can hold one of two values: \\(0\\) or \\(1\\). Every number, letter, image, or instruction is ultimately encoded as a sequence of bits.</p> <p>With two bits, there are <code>2\u00d72=4</code> possible combinations: <code>00</code>, <code>01</code>, <code>10</code>, and <code>11</code>. With three bits, there are <code>2\u00d72\u00d72=8</code> combinations: <code>000</code>, <code>001</code>, <code>010</code>, <code>011</code>, <code>100</code>, <code>101</code>, <code>110</code>, and <code>111</code>. In general, with \\(N\\) bits, the total number of combinations is \\(2^N\\). Each additional bit doubles the range of values that can be represented.</p> <p>Eight bits grouped together form a byte, the standard unit for representing a single character or a small piece of data. For example, the binary sequence <code>01000001</code> represents the letter \\(A\\)  in ASCII. Larger quantities of data are measured in multiples of bytes: kilobytes (KB), megabytes (MB), gigabytes (GB), and beyond.</p> Data Unit Size Data Unit Size Bit (b) \\(1\\) Byte (B) \\(8\\) bits Kilobyte (KB) \\(10^3\\) bytes Megabyte (MB) \\(10^{6}\\) bytes Gigabyte (GB) \\(10^{9}\\) bytes Terabyte (TB) \\(10^{12}\\) bytes Petabyte (PB) \\(10^{15}\\) bytes Exabyte (EB) \\(10^{18}\\) bytes Zettabyte (ZB) \\(10^{21}\\) bytes Yottabyte (YB) \\(10^{24}\\) bytes <p>Exercise</p> <p>How many different colors can be represented in an RGB image if each of the three color channels (Red, Green, Blue) is stored using 8 bits?</p>"},{"location":"Lecture_Notes/02_binary/#number-systems","title":"Number Systems","text":"<p>Every piece of data inside a computer \u2014 numbers, letters, images, even videos \u2014 is represented using number systems. A number system defines how we represent and interpret numerical values using a specific set of symbols (digits) and a base that indicates how many symbols are available.</p> System Base Digits Used Example Usage Decimal 10 0\u20139 245\u2081\u2080 Everyday counting and arithmetic. Binary 2 0, 1 101101\u2082 Used internally by digital computers. Hexadecimal 16 0\u20139, A\u2013F 2AF\u2081\u2086 Memory addresses, machine code, colors, compact notation. <p>Note</p> <p>In decimal, binary, hexadecimal systems, each position represents a power of 2, 10, 16, respectively.Hexadecimal system uses 0\u20139 and A\u2013F for values 10\u201315. Each hex digit equals 4 binary bits.</p> <ul> <li>245\u2081\u2080 = (2\u00d710\u00b2) + (4\u00d710\u00b9) + (5\u00d710\u2070) = 200 + 40 + 5 = 245\u2081\u2080</li> <li>1011\u2082 = (1\u00d72\u00b3) + (0\u00d72\u00b2) + (1\u00d72\u00b9) + (1\u00d72\u2070) = 8 + 0 + 2 + 1 = 11\u2081\u2080</li> <li>2AF\u2081\u2086 = (2\u00d716\u00b2) + (A\u00d716\u00b9) + (F\u00d716\u2070) = (2\u00d7256) + (10\u00d716) + (15\u00d71) = 687\u2081\u2080</li> </ul> Decimal Binary Hex Decimal Binary Hex 0 0000 0 8 1000 8 1 0001 1 9 1001 9 2 0010 2 10 1010 A 3 0011 3 11 1011 B 4 0100 4 12 1100 C 5 0101 5 13 1101 D 6 0110 6 14 1110 E 7 0111 7 15 1111 F <p>Note</p> <p>Decimal \u2192 Binary: Convert 13\u2081\u2080 by writing it as powers of two: 8+4+1=13. Hence 1101\u2082.</p> <p>Binary \u2192 Decimal: 1011\u2082 means 8+2+1=11, so it is 11\u2081\u2080.</p> <p>Binary \u2194 Hexadecimal: Group into 4-bit chunks: 10101110\u2082 = 1010\u2082 1110\u2082 = AE\u2081\u2086.</p> <p>Exercise</p> <p>Write down different numbers and convert back and forth between decimal, binary, and hexadecimal.</p>"},{"location":"Lecture_Notes/02_binary/#twos-complement","title":"Two\u2019s Complement","text":"<p>Just like in the decimal system, numbers in binary can be added together. Since binary uses only \\(0\\) and \\(1\\), the addition rules are simple. Binary addition is a core operation performed by the Arithmetic Logic Unit (ALU) in a CPU.</p> Addition Result Carry 0 + 0 0 0 0 + 1 1 0 1 + 0 1 0 1 + 1 0 1 <p>In digital systems, subtraction is often performed using addition: \\(A \u2212 B = A + (\u2212B)\\). To make this possible, computers represent negative numbers in two's complement form, allowing a single adder circuit to handle both addition and subtraction.</p> <p>With 3 bits, there are \\(2^3 = 8\\) different bit patterns. How we interpret those patterns depends on whether we want only positive numbers or both positive and negative. If we choose unsigned representation, all 3 bits are used for positive numbers. The patterns go from <code>000</code> to <code>111</code>, which correspond to the numbers \\(0\\) through \\(7\\).</p> <p>If we decide to include negative numbers, we can, for example, use one bit as a sign. The first bit indicates whether the number is positive (0) or negative (1), and the remaining two bits show the magnitude: <code>000</code> represents \\(+0\\), <code>001</code> is \\(+1\\), <code>010</code> is \\(+2\\), <code>011</code> is \\(+3\\), and the negative side goes from <code>100</code> (-0) to <code>111</code> (-3). This wastes one code for \\(-0\\) and makes arithmetic complicated and inefficient (try subtracting \\(2\\) from \\(1\\) in with sign bit representation). Hence, the na\u00efve \"sign bit\" approach (one sign bit + magnitude bits) creates two zeros (\\(+0\\) and \\(\u22120\\)) and breaks arithmetic in practice. </p> <p>Two's complement avoids these issues. With 3 bits, positives go from <code>000</code> (0) to <code>011</code> (3), and negatives go from <code>100</code> (\u22124) to <code>111</code> (\u22121). There is only one zero, and addition/subtraction works naturally. To find the negative of a number in two\u2019s complement:</p> <ol> <li>Invert the bits</li> <li>Add 1</li> <li>Discard any overflow carry</li> </ol>"},{"location":"Lecture_Notes/02_binary/#example-1-positive-result","title":"Example 1: Positive result","text":"<p>\\(A=7\\), \\(B=5\\). Compute \\(A - B\\) using 4-bit binary.</p> Step Operation Result 1 Write in binary A = 0111\u2082, B = 0101\u2082 2 Two\u2019s complement of B 0101 \u2192 1010 \u2192 1010 + 0001 = 1011\u2082 3 Add 0111 + 1011 = 10010\u2082 4 Discard carry 0010\u2082 5 Convert 2\u2081\u2080"},{"location":"Lecture_Notes/02_binary/#example-2-negative-result","title":"Example 2: Negative result","text":"<p>Compute \\(B\u2212A\\) using 4-bit binary.</p> Step Operation Result 1 Write in binary 5 = 0101\u2082, 7 = 0111\u2082 2 Two\u2019s complement of 7 0111 \u2192 1000 \u2192 1000 + 0001 = 1001\u2082 3 Add 0101 + 1001 = 1110\u2082 4 Interpret as negative 1110\u2082 is negative in 4-bit two\u2019s complement 5 Magnitude (optional) 1110 \u2192 0001 \u2192 0001 + 1 = 0010\u2082 \u2192 2\u2081\u2080, so result is \u22122\u2081\u2080 <p>Exercise</p> <p>Subtract any two numbers in binary using two\u2019s complement.</p>"},{"location":"Lecture_Notes/02_binary/#transistors-and-moores-law","title":"Transistors and Moore\u2019s Law","text":"By Omegatron.     The SVG code is     valid.     This symbol was created with     Inkscape.     CC BY-SA 3.0,     Link <p>Modern computer chips contain billions of transistors packed into an area smaller than a fingernail. In 1965, Intel co-founder Gordon Moore observed that the number of transistors on a chip was roughly doubling every two years, which became known as Moore\u2019s Law (it is not a physical law). This trend made computers faster, smaller, and cheaper for decades.</p> <p>Note</p> <p>Transistor scaling has slowed due to physical and engineering constraints such as heat dissipation (power density), leakage currents at very small geometries, and the increasing difficulty and cost of manufacturing at nanometer scales.</p>"},{"location":"Lecture_Notes/02_binary/#logic-gates","title":"Logic Gates","text":"<p>All digital logic is based on Boolean algebra, named after George Boole. In this system, everything is either <code>true</code> or <code>false</code>, or in computer terms, \\(1\\) or \\(0\\). Logic operations are implemented using logic gates, built inside a processor. Every complex calculation - from adding numbers to running software - comes from millions of tiny logical steps happening very fast.</p>        NOT gate, By Inductiveload \u2013       Own work, Public Domain,       Link        AND gate, By Inductiveload \u2013       Own work, Public Domain,       Link        OR gate, By Inductiveload \u2013       Own work, Public Domain,       Link        XOR gate, By Inductiveload \u2013       Own work, Public Domain,       Link <p>Truth tables are a precise way to describe how a logical operation behaves. They list all possible input values (usually 0 and 1) and show the exact output produced for each case. This makes the behavior of a logic gate or logical rule completely unambiguous and easy to verify. Below are standard truth tables for basic logic gates.</p> <p>The NOT gate inverts its input.</p> A Q 0 1 1 0 <p>Both AND gate and OR gate take two inputs. AND outputs 1 only if both inputs are 1. OR outputs 1 if at least one input is 1.</p> A B AND (Q) OR (Q) 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 <p>The XOR (exclusive OR) gate outputs 1 only when the inputs are different.</p> A B Q 0 0 0 0 1 1 1 0 1 1 1 0 <p>Exercise</p> <p>How would you implement XOR using only NOT, AND, and OR? You can use an online digital logic simulator.</p> <p>Exercise</p> <p>Use logic gates to add any two single-bit binary numbers. Which gates would you use? Hint: use the addition table provided in the section dedicated to \"two's complement\".</p>"},{"location":"Lecture_Notes/02_binary/#additional-material","title":"Additional Material","text":"<ul> <li>Early Computing: Crash Course Computer Science #1</li> <li>Electronic Computing: Crash Course Computer Science #2</li> <li>Boolean Logic &amp; Logic Gates: Crash Course Computer Science #3</li> <li>Binary: Plusses &amp; Minuses (Why We Use Two\u2019s Complement) - Computerphile</li> <li>Why It Was Almost Impossible to Make the Blue LED</li> </ul>"},{"location":"Lecture_Notes/03_hardware/","title":"03. Hardware: CPU, GPU, Memory, Storage Devices","text":"<p> Rahida Asadli, Ismayil Shahaliyev Oct 25, 2025 Jan 30, 2026</p>"},{"location":"Lecture_Notes/03_hardware/#hardware","title":"Hardware","text":"<p>Hardware refers to the physical components of a computer system, the parts you can see and touch. It includes all electronic and mechanical elements that work together to input, process, store, and output data.</p> <p>The motherboard is the main circuit board and backbone of the computer, responsible for connecting and coordinating all hardware components. It provides electrical pathways (called buses) and controllers that allow the Central Processing Unit (CPU), memory, storage devices, input components (e.g. keyboard, mouse, microphone, scanner), and output components (e.g. monitor, printer, speaker) to communicate quickly and in the correct sequence. The motherboard ensures that data flows smoothly between these parts, much like a central highway system linking different parts of a city. The CPU socket is the slot where the processor is installed; through tiny metal contacts, it connects directly to the motherboard so the CPU can fetch, decode, and execute instructions from memory. Beside it are the memory slots, where Random-Access Memory (RAM) modules are inserted. RAM temporarily holds the data and instructions the CPU is currently working with, allowing fast access and efficient processing.</p>      The motherboard of a Samsung Galaxy SII; almost all functions of the device are integrated into a very small board. By     Vera de Kok \u2013     Own work,     CC BY-SA 3.0,     Link <p>Note</p> <p>Imagine you open the calculator app on your computer. When you click the icon, the action is sent through the motherboard. The calculator program is loaded into RAM, and the CPU begins reading its instructions from memory and carrying them out. When you type numbers, they are kept in RAM while the CPU performs the calculations. The result is then shown on the screen. If you close the app, the data in RAM is cleared, but the calculator program itself remains stored in a storage device for future use.</p>"},{"location":"Lecture_Notes/03_hardware/#von-neumann-architecture","title":"Von Neumann Architecture","text":"<p>The von Neumann architecture (1945) describes how most computers are structured. It consists of five key components. The input unit receives data and instructions from external devices. The memory unit stores data and instructions, either temporarily during processing or permanently for long-term use. The arithmetic logic unit performs arithmetic operations, such as addition and subtraction, as well as logical operations. The control unit directs and coordinates all system activities by controlling the flow of data and instructions between components. Finally, the output unit presents the processed information to the user or transmits it to other systems through output devices or communication interfaces.</p>      A von Neumann architecture scheme. By     Kapooht \u2013     Own work,     CC BY-SA 3.0,     Link <p>Both data and instructions are stored together in the same memory. This is called the stored-program concept. It was important because it allowed computers to keep programs in memory along with the data they use. This made it possible to change or run different programs by loading new instructions into memory, instead of manually changing the computer\u2019s hardware connections. It simplified computer design, enabled automation of complex tasks, and made programming far more flexible.</p> <p>This seems obvious today, but only because it became the foundation of modern computing. Before the stored-program concept, computers like ENIAC had to be rewired by hand for every new task. There was no \u201cprogram\u201d to load\u2014the hardware was the program. John von Neumann\u2019s idea separated hardware (the machine) from software (the instructions it runs) and treated code as just another form of data. That shift made general-purpose computers possible.</p> <p>Note</p> <p>When you use a calculator app to add two numbers, both the program\u2019s instructions (load number, add, display result) and the numbers you enter are stored in RAM. The CPU retrieves and executes the instructions one by one, reads the data from memory, performs the addition, and writes the result back, following the stored-program model.</p>"},{"location":"Lecture_Notes/03_hardware/#central-processing-unit","title":"Central Processing Unit","text":"<p>Central Processing Unit (CPU) follows program instructions and performs the steps needed to complete tasks, from typing a word to playing music. The CPU also controls and coordinates the work of other parts of the computer, making sure everything happens in the right order. It has three main components.</p>      A high-end consumer CPU made by Intel: an     Intel Core i9-14900KF.     By Pstrahl \u2013     Own work,     CC BY-SA 4.0,     Link <p>Control Unit (CU) manages and directs activities inside the CPU. It tells the computer when to fetch data, when to carry out an instruction, and where to send results. You can think of it as the traffic controller of the CPU.</p> <p>Arithmetic Logic Unit (ALU) performs arithmetic operations (addition, subtraction, multiplication, division) and logical operations (comparisons such as equal to, greater than, AND, OR, NOT). When the CPU executes an instruction like <code>add 2 + 3</code>, the CU sends the task to the ALU. The ALU performs the calculation and sends the result back to registers or memory.</p> <p>Registers are tiny, high-speed memory locations built directly into the CPU. They store data and instructions temporarily during processing. Because they are inside the CPU, registers are much faster than RAM. Examples include:</p> <ul> <li>Instruction Register (IR) holds the current instruction being executed.</li> <li>Program Counter (PC) keeps track of the next instruction\u2019s address in memory.</li> <li>Accumulator (ACC) stores intermediate arithmetic and logic results.</li> </ul> <p>Every action the CPU performs follows four main steps, known as the machine cycle:</p> <ul> <li>Fetch. CPU gets an instruction from RAM.</li> <li>Decode. CPU determines what the instruction means.</li> <li>Execute. CPU carries out the action (the ALU does the work).</li> <li>Store. CPU saves the result back to memory or sends it to an output device.</li> </ul> <p>This process (<code>fetch \u2192 decode \u2192 execute \u2192 store</code>) repeats continuously, millions or billions of times per second.</p> <p>Note</p> <p>When a computer adds two numbers, several CPU components work together. First, the CU fetches the instruction from main memory (the binary code telling the CPU what to do). Then it decodes the instruction and identifies which data to use. After decoding, the CU sends control signals that tell the ALU to perform the operation and tell the registers when to load or output data.</p> <p>The ALU performs the actual arithmetic (e.g. addition) or logical operations. It receives the two numbers from the registers, adds them in binary form, and produces the result. Meanwhile, the registers act as small, extremely fast storage locations inside the CPU. Before the ALU starts, the numbers to be added are placed in registers. After the ALU finishes, the result is stored in another register, ready to be sent back to main memory or used for the next operation.</p> <p>Throughout the process, the CU ensures proper timing and order, making sure that data moves only when it should and that all components work in sync. This precise coordination - fetching, decoding, executing, and storing - is what allows even the simplest task, such as adding two numbers, to happen billions of times per second without error.</p> <p>The clock is the timing system of the CPU. It sends out a steady stream of electrical pulses that set the pace for operations. The clock speed indicates how many cycles occur per second and is measured in Hertz (Hz). For example, a CPU with a 3.0 GHz clock runs about 3 billion cycles per second. A cycle is not the same as an instruction; modern CPUs may execute multiple instructions per cycle or require multiple cycles per instruction. The clock ensures that all parts of the CPU work together in perfect rhythm.</p> <p>Word size is the number of bits the CPU can process in one operation. The most common sizes are 32-bit and 64-bit. A 64-bit CPU can handle larger data chunks and can address much more memory than a 32-bit CPU.</p> <p>Modern CPUs contain multiple cores, meaning multiple processing units inside a single chip. Each core can run instructions independently, allowing parallel processing. A dual-core CPU can run two heavy tasks at once (for example, encoding a video while running a game), while lighter tasks can be managed by time-sharing even on a single core. A quad-core CPU can handle four heavy tasks, and an octa-core CPU can handle eight - ideal for heavy work such as gaming or video editing.</p> <p>Instruction Set Architecture (ISA) is the built-in language of the CPU. It defines the commands the CPU can understand and execute. The ISA specifies what operations exist and how programs interact with memory and hardware, while the microarchitecture describes how a particular CPU implements and executes those instructions internally. Common examples include x86/x86-64 (Intel, AMD) and ARM (mobile devices and many modern laptops).</p> <p>Note</p> <p>Just as people speak different languages, an Intel CPU \"speaks\" x86 instructions, while a smartphone CPU \"speaks\" ARM. Programs must be written or compiled in the correct instruction set so the CPU understands them.</p>"},{"location":"Lecture_Notes/03_hardware/#graphics-processing-unit","title":"Graphics Processing Unit","text":"<p>While the CPU is responsible for general-purpose computation and overall control of the system, modern computers also rely heavily on the Graphics Processing Unit (GPU). A GPU is a specialized processor designed to perform a very large number of simple calculations in parallel. Originally, GPUs were created to handle graphics tasks such as drawing images, rendering 3D scenes, and updating the screen smoothly in video games and graphical applications.</p> <p>Unlike the CPU, which has a small number of powerful cores optimized for sequential and complex tasks, a GPU contains thousands of smaller, simpler cores optimized for parallel work. This makes GPUs extremely efficient at tasks where the same operation must be applied to many data elements at once, such as processing pixels, vertices, or vectors. For example, when rendering an image, a GPU can compute the color of millions of pixels simultaneously, something a CPU would do much more slowly.</p> <p>Today, GPUs are important far beyond graphics. They are widely used in scientific computing, simulations, video processing, and especially artificial intelligence and machine learning, where large matrix and vector operations are required. In these cases, the CPU typically prepares tasks and manages control flow, while the GPU performs the heavy numerical computations. Together, the CPU and GPU complement each other: the CPU handles decision-making and coordination, and the GPU provides massive computational throughput for parallel workloads.</p>"},{"location":"Lecture_Notes/03_hardware/#memory-devices","title":"Memory Devices","text":"<p>Every computer must store data and instructions so the CPU can use them. The place where this information is kept is called memory. Different kinds of memory serve different purposes - some are very fast but volatile (temporary), while others are slower but non-volatile (permanent). The contents of volatile memory are lost when power is turned off, which is not the case for non-volatile memory.</p> <p>The classification of computer storage into three main categories is a conventional way of organizing memory types based on their speed, volatility, access patterns, and typical use. Primary storage is volatile memory used directly by the CPU for immediate processing and includes registers, cache memory, and main memory, holding the data and instructions currently being executed. Secondary storage is non-volatile and provides persistent storage for programs and data, which must be loaded into primary memory before use. Tertiary storage is also non-volatile but much slower, mainly used for long-term backups and archives, accessed infrequently and often relying on sequential access, with examples including optical discs<sup>1</sup> (e.g. CD/DVD) and magnetic tapes that offer very large capacity at low cost.</p>"},{"location":"Lecture_Notes/03_hardware/#primary-storage","title":"Primary Storage","text":"<p>RAM (Random Access Memory) is the computer's main working memory. It temporarily stores data and instructions that the CPU is currently using, allowing quick access for ongoing tasks. RAM is volatile and has two main types: Dynamic RAM (DRAM) and Static RAM (SRAM).</p> <ul> <li>DRAM stores each bit of data in a tiny capacitor that holds an electrical charge - charged for 1, empty for 0. Because these capacitors leak energy, DRAM must be constantly refreshed to retain data. Its simple design (just one transistor and one capacitor per bit) allows it to hold large amounts of data at a low cost, but this also makes it slower than other memory types. DRAM serves as the main memory in computers, holding the programs and data currently loaded and being used by the CPU, like open browser tabs or a running video, so the CPU can access them quickly.</li> <li>SRAM uses flip-flop circuits made of transistors that keep data stable as long as power is on - no refreshing needed. This makes SRAM much faster and more reliable, but also larger and more expensive.</li> </ul> <p>Cache memory is a very small, ultra-fast SRAM built directly inside the CPU. It stores data and instructions that are used often, so the CPU doesn't have to fetch them repeatedly from the slower main memory (RAM). L1 cache is the smallest and fastest, located inside each CPU core. L2 cache is larger and slightly slower, typically dedicated to each core. L3 cache is the largest and slowest cache level, shared among all cores in the processor.</p> <p>Note</p> <p>Imagine you're working on a laptop with several browser tabs open - one for email, one for an online document, and one for a video. All of these open tabs and the data they use are stored in DRAM, which holds the programs and information you are currently using so the CPU can access them quickly. When you switch between tabs, the CPU retrieves the required data directly from DRAM. Because DRAM is volatile, all open tabs and unsaved work are lost if power is removed.</p> <p>At the same time, the CPU relies on much smaller but faster SRAM inside the processor, known as cache memory. The cache stores the most frequently used instructions and small pieces of data needed while those browser tabs are running. For example, when parts of a webpage or video are accessed repeatedly, the CPU keeps them in cache so they can be reused immediately without waiting for DRAM. In this way, DRAM functions like a large work surface holding everything you are working on, while SRAM cache is like the few critical notes kept right at hand for instant access.</p> <p>Read-Only Memory (ROM)<sup>2</sup> is a non-volatile memory. Unlike RAM, it retains data even when power is off. When the computer is turned on, the CPU reads the firmware stored in ROM - known as the BIOS or UEFI, which checks the hardware (e.g. memory, keyboard, and drives) and then loads the operating system into main memory. Historically, ROM was preprogrammed and not easily changed. In modern systems, firmware is stored in flash memory (a type of Electrically Erasable Programmable ROM) and can be updated by the user.</p>"},{"location":"Lecture_Notes/03_hardware/#secondary-storage","title":"Secondary Storage","text":"<p>Storage devices use two main data-retrieval methods: sequential access and direct (random) access. In sequential access, data is read in a fixed order from the beginning until the desired location is reached, which makes access slower but the system simpler and cheaper to implement. In random access, the device can jump directly to any data location without reading preceding data, resulting in much faster access.</p> <p>Hard Disk Drive (HDD) stores data using magnetic platters that spin at very high speeds, usually between 5,400 and 7,200 revolutions per minute (RPM). Data is written and read by a tiny magnetic head that moves across the surface of the spinning disks without touching them. Because of its mechanical structure, an HDD can hold a large amount of data - commonly ranging from 1 to over 20 terabytes. However, since the disk and read/write head involve physical movement, they are relatively slow and can wear out over time and are more vulnerable to shocks or drops. The device allows random access.</p> <p>Solid State Drive (SSD) stores data using flash memory chips and has no moving parts, which makes it much faster and more reliable than traditional hard drives. Data is stored electronically in millions of tiny transistors_,_ similar to the technology used in USB flash drives, but with far faster interfaces and controllers. SSDs provide very fast boot times, quick application loading, and instant file access, which is why they are now the standard in most laptops and modern computers. Since there are no spinning disks or moving heads, SSDs are also silent, lighter, and more resistant to physical shock, making them ideal for portable devices. However, their cost per gigabyte is higher than that of HDDs. SSDs use electronic random access and are much faster than HDDs, which rely on mechanical movement and are slow for random access.</p>      Storage hierarchy pyramid (created with     matplotlib).    <p>To sum up, the storage hierarchy shows how different types of memory and storage are organized based on speed, cost, and capacity. At the top of the pyramid is cache memory; it is the fastest and most expensive per unit of storage, but also the smallest in size. Cache resides on the CPU chip and provides data almost instantly. Below it is main memory (RAM), which is slightly slower and cheaper, and is used to store data that the CPU is currently working on. Moving further down, flash storage (such as solid-state drives) is non-volatile, meaning it retains data when power is off. It is slower than RAM but much faster than magnetic or optical storage. Next comes magnetic disk storage (hard drives), which provides large capacity at a lower cost, but has slower access due to mechanical movement. Below that are optical discs (such as CDs, DVDs, and Blu-ray), mainly used for media distribution or backups; they are inexpensive but slower and more limited in capacity compared to disks. At the bottom of the pyramid is magnetic tape, which stores massive amounts of data very cheaply but is extremely slow because it relies on sequential access rather than direct access. As you move upward in the hierarchy, speed and cost increase while capacity decreases; as you move downward, capacity increases and cost per bit decreases, but access time becomes slower.</p>"},{"location":"Lecture_Notes/03_hardware/#additional-material","title":"Additional Material","text":"<ul> <li>How do Transistors Build into a CPU? How do Transistors Work?</li> <li>How do Hard Disk Drives Work?</li> <li>How does Computer Memory Work?</li> <li>How Computers Calculate - the ALU: Crash Course Computer Science #5</li> <li>How a CPU Works in 100 Seconds</li> <li>Von Neumann Architecture - Computerphile</li> <li>How do Graphics Cards Work? Exploring GPU Architecture</li> </ul> <ol> <li> <p>Disk is standard in computer science for magnetic and solid-state storage (hard disk, disk drive). Disc is used for optical media (CD/DVD/Blu-ray discs).\u00a0\u21a9</p> </li> <li> <p>ROM is not used as working memory and is best treated separately from primary/secondary/tertiary storage. It stores non-volatile firmware used mainly during system startup rather than data and instructions actively processed by the CPU.\u00a0\u21a9</p> </li> </ol>"},{"location":"Lecture_Notes/04_algorithm/","title":"04. Algorithm &amp; Algorithmic Actions","text":"<p> Nilufar Ismayilova, Ismayil Shahaliyev Oct 25, 2025 Jan 30, 2026</p> <p>An algorithm is a step-by-step procedure or set of rules to solve a specific problem or perform a task. It is like a recipe in cooking: you follow a clear sequence of actions to achieve a specific result. In computer science, algorithms receive some input, process it in a logical way, and produce an output. What makes a good algorithm is that it has a clear starting point, a clear ending point, and every step is unambiguous - there is no confusion about how it should be executed.</p> <p>Note</p> <p>Imagine you want to find the largest of three numbers: A, B, and C. A simple algorithm would say: first compare A and B. If A is greater than or equal to B, then compare A with C. If A is still the greatest, then A is the largest number. Otherwise, C is the largest. But if in the very first comparison B was already greater than A, then compare B with C, and whichever is larger is the answer. This exact set of steps can be written in code or performed by a person.</p> <p>Exercise</p> <p>Bring an example of an algorithm from your daily life.</p> <p>The word algorithm traces back to the name of the mathematician Al-Khwarizmi, whose systematic approach to calculation and problem-solving became foundational to modern mathematics and computing. Similarly, the word algebra comes from his mathematical treatise Al-Jabr, translated as \"completion\" or \"rejoining\".</p> <p>Turing Award winner computer scientist Donald Knuth is best known for the book The Art of Computer Programming. His work helped establish algorithmic analysis as a discipline.</p> <p>Computational complexity of an algorithm describes how the resources an algorithm needs (e.g. time, memory, etc.) grow as the input size increases. Time complexity tracks how many computational steps are required. Space complexity tracks how much memory is used. Within these, we distinguish worst-case, average-case, and best-case behavior depending on how the input might vary. Asymptotic (Big O) notation provides a language for comparing growth rates when inputs become large. Big O gives an upper bound: it states that the algorithm grows no faster than some function up to constant factors. Big O abstracts away hardware, implementation details, and constants, focusing only on how the algorithm scales with input size.</p> <p>Knuth also wrote a special foreword publication for participants of the International Olympiad in Informatics (IOI 2019) held in Azerbaijan. A Book of Tasty Algorithms is a playful recipe book based on dishes from Azerbaijani cuisine, representing algorithmic ideas through the familiar structure of cooking, drawing a parallel between writing algorithms and following recipes.</p>"},{"location":"Lecture_Notes/04_algorithm/#algorithmic-actions","title":"Algorithmic Actions","text":"<p>Algorithmic actions describe the fundamental operations that make up any computer program. These actions determine how a program makes decisions, repeats steps, organizes tasks, and manages data. The five main algorithmic actions are selection, repetition, modularization, recursion, and name binding.</p>"},{"location":"Lecture_Notes/04_algorithm/#selection","title":"Selection","text":"<p>Selection means choosing between different paths of execution based on a condition. It allows the program to \"decide\" what to do depending on input or data values. In the pseudo-code below, if the temperature is above 30\u00b0C, the system turns on the air conditioner; otherwise, it keeps it off:</p> <pre><code>if (temperature &gt; 30) {\n  turnOnAC();\n} else {\n  turnOffAC();\n}\n</code></pre>"},{"location":"Lecture_Notes/04_algorithm/#repetition","title":"Repetition","text":"<p>Repetition means performing the same set of instructions multiple times until a certain condition is met. This prevents code duplication and makes programs efficient. In programming (pseudo-code), printing numbers from 1 to 5 using a loop will look like:</p> <pre><code>for (int i = 1; i &lt;= 5; i++) {\n  print(i);\n}\n</code></pre>"},{"location":"Lecture_Notes/04_algorithm/#modularization","title":"Modularization","text":"<p>Modularization means dividing a complex program into smaller, manageable, and reusable parts, called modules. This improves readability, debugging, and reusability.</p> <p>Note</p> <p>A billing program may have modules <code>calculateTax()</code> and <code>printBill()</code> for computing tax on a purchase and printing the final receipt. Here, each module performs one well-defined task, making the program organized and easier to maintain.</p> <p>At a deeper level, modularization is a principle of system organization, not limited to programming. It refers to the act of decomposing any complex system - software, mechanical, social, or educational - into smaller subsystems that can be developed, understood, or replaced independently.</p> <p>In programming, modularization starts with functions, which perform a single, well-defined task. Related functions can be grouped into classes, which represent objects or abstract entities. Several classes and functions together form a module, typically a single file in a programming language like Python. A package is a collection of related modules organized in a directory (folder) structure, and multiple packages can form a library or framework, representing a higher level of modular organization. At the top level, systems or applications integrate many libraries and frameworks, sometimes developed by entirely different teams or companies.</p> <pre><code>class Calculator:\n    function add(a, b):\n        return a + b\n    function subtract(a, b):\n        return a - b\n</code></pre> <p>In engineering, a car is designed in modules such as the engine, transmission, and electrical system, each of which can be developed or replaced independently. In education, a curriculum may be modularized into separate courses or subjects, each focusing on a distinct domain of knowledge but contributing to an integrated learning goal. In management, an organization may be divided into departments - finance, marketing, research, operations - each acting as a module with a specific role but coordinated under one structure. In architecture, buildings are designed with modular components such as prefabricated walls or units that can be rearranged or replaced.</p> <p>Modularization is about complexity management: dividing a whole into parts that are independent in function and cooperative in purpose. At even higher levels, modularization can be systemic or societal. Complex infrastructures - transportation networks, communication systems, and governance structures - are modularized into interacting parts so that local failures do not collapse the entire system.</p> <p>Exercise</p> <p>Take a major goal of yours for the next six months and divide it into manageable modules.</p>"},{"location":"Lecture_Notes/04_algorithm/#recursion","title":"Recursion","text":"<p>Recursion occurs when a function calls itself to solve smaller parts of the same problem. Each recursive step reduces the problem size until it reaches a base case, the simplest situation where the function stops calling itself. The base case is not the starting point, it is the stopping condition</p> <p>Note</p> <p>You can imagine real-life recursion: a tree has branches, branches have smaller branches, those branches have their own smaller branches, etc. But this recursion may go infinitely and you usually need to stop at some point. That stopping condition is the base case. Let's say, after four-five branching steps a tree will stop growing any branches. With that logic, there is no base case in the case of two mirrors oppositely directed to each other. There is no base case and you will see infinite reflections. Or: infinite recursion.</p> <p>We know that the factorial of <code>n</code> is the factorial of the previous number multiplied by <code>n</code>. That means: <pre><code>factorial(5) = factorial(4) * 5\nfactorial(4) = factorial(3) * 4\nfactorial(3) = factorial(2) * 3\nfactorial(2) = factorial(1) * 2\n</code></pre></p> <p>Or, as a general case:</p> <pre><code>factorial(n) = factorial(n-1) * n\n</code></pre> <p>We can write the following recursive function:</p> <pre><code>factorial(n) {\n  return n * factorial(n - 1);\n}\n</code></pre> <p>But this pseudo-code has an issue. It is akin to two mirrors looking at each other - it will never stop. It will return the following functions:</p> <pre><code>factorial(1) = factorial(0) * 1\nfactorial(0) = factorial(-1) * 0\nfactorial(-1) = factorial(-2) * -1\n\n// ad infinitum\n</code></pre> <p>Not only <code>factorial(-1)</code> doesn't make sense, but also you will be in an infinite recursion. To fix it, you need to add a halting (stopping) condition: Base case.</p> <pre><code>factorial(n) {\n  if (n == 1 or n == 0) return 1; // base case\n  else return n * factorial(n - 1); // recursive call\n}\n</code></pre> <p>So, your code will work in the following way, given that <code>n</code> is a non-negative number (in this case <code>n=4</code>): </p> <pre><code>factorial(4) = factorial(3) * 4 = factorial(2) * 3 * 4 = factorial(1) * 2 * 3 * 4 = 1 * 2 * 3 * 4 = 24.\n</code></pre> <p>Exercise</p> <p>Write a pseudo-code for recursive algorithm to find the sum of natural numbers.</p>"},{"location":"Lecture_Notes/04_algorithm/#name-binding","title":"Name Binding","text":"<p>Name binding refers to the process of connecting identifiers (names) to specific objects, values, or memory locations in a program. It defines where and when a variable's name is linked to the data it represents. This action allows the program to store, access, and update information consistently.</p> <p>When a program needs to store the value <code>70</code>, it allocates memory to represent it. For example in memory cell represented with hexadecimal number <code>0x21A95BC</code>, bytes will be allocated to represent <code>70</code> in binary. Then, if you want to reuse that value in the future, you need to assign (not equalize) that value to an identifier. </p> <pre><code>speed = 70\n</code></pre> <p>The identifier speed becomes bound to the value <code>70</code> stored in memory. If later the value changes (<code>speed = 50</code>), the same name now refers to a different value, possibly stored in a different memory location.</p>"},{"location":"Lecture_Notes/04_algorithm/#dijkstras-shortest-path-algorithm","title":"Dijkstra's Shortest Path Algorithm","text":"<p>In many information systems, a common task is to search for an item. In some systems, the problem involves navigating networks, such as transportation systems or computer networks, where components are connected by links with different costs, delays, or capacities. In these cases, the goal is not merely to locate an item, but to determine an optimal path through the network.</p> <p>Edsger W. Dijkstra was a Dutch computer scientist and one of the founders of modern computer science. Dijkstra's algorithm is a classic solution that computes the shortest path from a starting node to all other nodes in a weighted graph where all edge weights are non-negative. The algorithm maintains a set of nodes whose minimum distance from the start is already known. At each step, it selects the node with the smallest temporary distance, finalizes its distance, and then updates (relaxes) its neighbors by checking whether passing through this node yields a shorter path.</p>      Dijkstra\u2019s algorithm to find the shortest path between a and b. It picks the unvisited vertex with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor\u2019s distance if smaller. Mark visited (set to red) when done with neighbors. By     Ibmua \u2013     Work by uploader, Public Domain,     Link <p>A priority queue is often used to efficiently select the next node with the smallest distance, which allows the algorithm to scale well for large graphs. The algorithm finishes when all nodes have been processed or when the destination node is finalized. The output is both the shortest distance and the actual path taken.</p>"},{"location":"Lecture_Notes/04_algorithm/#additional-material","title":"Additional Material","text":"<ul> <li>Intro to Algorithms: Crash Course Computer Science #13</li> <li>What's an algorithm? - David J. Malan</li> <li>What on Earth is Recursion? - Computerphile</li> <li>Binary, Hanoi and Sierpinski, part 1</li> <li>Dijkstra's Algorithm - Computerphile</li> </ul>"},{"location":"Lecture_Notes/05_software/","title":"05. Programming &amp; Software","text":"<p> Rahida Asadli, Ismayil Shahaliyev Nov 9, 2025 Jan 31, 2026</p>"},{"location":"Lecture_Notes/05_software/#programming","title":"Programming","text":"<p>Programming is the process of taking an algorithm and writing it in a language that a computer can understand and execute. This language is called a programming language.</p> <p>A computer does not understand programming languages directly. It only understands machine code: very low-level instructions represented in binary (0s and 1s) that tell the CPU exactly what to do. A programming language (such as Python, C, Java, JavaScript, Lisp, or Prolog) is therefore a human-readable abstraction. It must be translated into machine code before the computer can run it.</p> <p>Every programming language has rules about how things must be written. These rules are called syntax. If you break the syntax rules, the computer will show an error and will not run the program. However, even if your syntax is correct, your program may still not work the way you expect. That is where semantics comes in. Semantics refers to the meaning behind each statement.</p> <p>Note</p> <p>The instruction <code>x = 5 + ;</code> is a syntax error because the grammar is wrong. But the instruction <code>x = \"Hello\" - 3</code> is syntactically correct but semantically meaningless, because subtracting a number from a word makes no logical sense.</p>"},{"location":"Lecture_Notes/05_software/#compiler-vs-interpreter-compiler-vs-interpreter","title":"Compiler vs Interpreter {#compiler-vs-interpreter}","text":"<p>A compiler is a program that translates the entire source code of a program (written in a high-level language like C++) into machine code or an intermediate representation close to machine code before the program runs. The output of this translation is usually an executable file (.exe, .out) or bytecode. Think of a compiler like a translator who reads a whole book, understands it fully, and then produces a translated version. After that, you can read the translated book without the translator.</p> <p>An interpreter reads the source code line by line (or instruction by instruction), translating and executing on the spot. It does not create a separate executable file. Think of an interpreter like a live translator who translates every sentence as the speaker is talking. If you want to hear the speech again, you need the interpreter present.</p> <p>Compiled programs usually run faster, because the translation work is done in advance, and many errors are detected before execution. However, compilation adds an extra step before running the program, and any change in the source code requires recompilation. The resulting executable is also often tied to a specific platform. Interpreted programs, on the other hand, typically start quickly and are easier to test and modify, since code can be run immediately without a separate build step. The downside is that they generally run more slowly, errors may appear only when a particular instruction is reached, and the program always depends on the presence of the interpreter at runtime.</p> <p>A compiler is typically used when building software that must deliver predictable high performance. For example, when developing an iOS application such as a fitness tracker or a game, smooth performance, fast startup, and good battery efficiency are important. In such cases, most of the program is compiled ahead of time into native machine code tailored to the phone's processor. This allows the operating system to run the application directly, without translating the program during execution. Because the executable can be distributed and run independently of the compiler, compiled languages such as Swift or C++ are well suited for performance-critical, widely distributed software.</p> <p>An interpreter is commonly used in situations where development speed, flexibility, and ease of experimentation are more important than maximum performance. For example, when automating a task like renaming files, collecting data from a website, or processing a spreadsheet, it is useful to write code and run it immediately without a separate build step. Interpreted environments allow programmers to test ideas incrementally and observe results right away, which makes debugging and iteration faster. For this reason, engineers, researchers, and students often use languages such as Python or JavaScript for scripting, data analysis, and rapid prototyping.</p> <p>Important</p> <p>Modern interpreted languages are not necessarily slow. Many, including JavaScript, use just-in-time compilation and runtime optimizations that translate frequently executed parts of the program into machine code while the program is running. As a result, the practical difference between compiled and interpreted systems is not simply speed, but when and how code is translated, and what trade-offs are made between performance, portability, and development convenience.</p> <p>Note</p> <p>Below is an example of an oversimplified workflow. Imagine you wrote a short program: <pre><code>print(\"Hello\")\nprint(10 + 5)\n</code></pre></p> <p>A compiler reads the entire program first, checks for syntax and many semantic errors, and then translates the whole code into machine language before running it. It is like translating an entire book from English to Azerbaijani, checking everything, and then giving the translated book to the reader.</p> <ol> <li>You write the complete program.</li> <li>The compiler analyzes the entire program at once.</li> <li>If no compile-time errors are found, it produces an executable program.</li> <li>Only after that does the program run and produce output.</li> </ol> <p>If a compile-time error exists anywhere in the program, the program will not run at all.</p> <p>An interpreter runs a program during execution, translating and executing instructions one at a time as the program proceeds. It is like a translator standing next to you: you speak a sentence in English, and the translator immediately says it in Azerbaijani. If the program is: <pre><code>print(\"Hello\")\nprint(10 + 5)\nprint(10 / 0) # error happens here\nprint(\"This will not run\")\n</code></pre></p> <p>An interpreter may do the following: 1. Run the first line \u2192 prints <code>Hello</code> 2. Run the second line \u2192 prints <code>15</code> 3. Run the third line \u2192 finds an error (division by zero) and stops.</p> <p>The last line will not run, but the first two lines still worked.</p>"},{"location":"Lecture_Notes/05_software/#evolution-of-programming-languages","title":"Evolution of Programming Languages","text":"<p>Programming languages have evolved over time to make communication between humans and computers easier. Early computers could only understand sequences of 0s and 1s (machine code), but modern languages allow us to write instructions in a way that is closer to human thinking. This development is divided into generations, where each generation makes programming more abstract, simpler, and more powerful than the previous one.</p> <p>First-generation (Machine Language). Consists only of binary digits 0s and 1s. Directly understood by the computer but extremely difficult for humans to write or debug.</p> <pre><code>10110000 00000101 (this may imply load 5 into register)\n10110001 00000110 \n00000001 (this may imply addition)\n</code></pre> <p>Second-generation (Assembly Language). Uses symbolic codes or abbreviations instead of binary. Easier than machine language, but still closely tied to hardware. Needs an assembler to convert to machine code.</p> <pre><code>MOV AX, 5 ; move 5 into register AX\nMOV BX, 6 ; move 6 into register BX\nADD AX, BX ; add BX to AX\n</code></pre> <p>Third-generation (High-Level Languages). Uses English-like words and mathematical symbols. Easier to read, write, and understand. Portable across different machines. See the Python code below.</p> <pre><code>a = 5\nb = 6\nsum = a + b\nprint(sum)\n</code></pre> <p>Fourth-generation (Very High-Level Languages). Designed to reduce programming effort. Focuses more on what needs to be done rather than how. Often used in databases and report generation. See <code>SQL</code> code below.</p> <pre><code>SELECT * FROM users WHERE name='Codd'\n</code></pre> <p>Fifth-generation (Logic-Based Languages). Based on formal logic, rules and facts, automated inference. The programmer specifies knowledge, not procedures. See Prolog code below.</p> <pre><code>parent(alice, bob).\nparent(bob, carol).\nparent(carol, dave).\nancestor(X, Y) :- parent(X, Y).\nancestor(X, Y) :- parent(X, Z), ancestor(Z, Y).\n</code></pre> <p>Example queries a user would ask the Prolog system: <pre><code>?- parent(alice, bob).\n?- ancestor(alice, carol).\n?- ancestor(alice, dave).\n?- ancestor(X, dave).\n</code></pre></p> <p>Post-Fifth Generation (Generative AI Systems). Modern computing introduces a different paradigm based on machine learning and generative models. In this approach, systems are not programmed with explicit rules or logical facts. Instead, they are trained on large amounts of data and learn statistical patterns that allow them to generate text, code, images, or actions in response to human instructions. Unlike logic-based systems, generative AI does not perform formal logical inference and does not guarantee correctness. Its strength lies in handling ambiguity, incomplete information, and complex real-world data at scale. Users typically interact with such systems using natural language prompts, examples, or goals, rather than writing precise programs. For this reason, generative AI is best understood not as a new programming language generation, but as a new interaction and computation paradigm built on top of existing languages and systems.</p>"},{"location":"Lecture_Notes/05_software/#software","title":"Software","text":"<p>Software consists of computer programs that instruct the computer what to do, and includes design documents and specifications (documentation). Software converts user commands into machine-level instructions and enables computers to perform everything from calculations to complex design simulations.</p> <p>System software manages and operates the computer hardware so that other programs can run. It provides a platform for application software. Below are types and examples of the system software.</p> <p>Note</p> <p>When you turn on a laptop, the operating system (e.g. Windows, macOS, Linux) loads first. It checks the keyboard, touchpad, and memory, and loads drivers for your hardware. Only then can you open applications such as Google Chrome, Microsoft Word, or VS Code.</p> Type Purpose Examples Operating Systems Manage hardware and software resources, provide user interface Windows, macOS, Linux Device Drivers Control and communicate with specific hardware devices Printer driver, GPU driver Utility Programs Perform maintenance and optimization tasks File Manager, Disk Cleanup, Antivirus <p>Note</p> <p>When you power on a smartphone, Android or iOS initializes the display, touchscreen, and sensors. After that, you can launch applications like WhatsApp, Spotify, or Instagram.</p> <p>Application software consists of programs designed for end users to perform particular functions. This includes everything from productivity tools to entertainment and educational programs (e.g., Microsoft Word, Adobe Photoshop, Spotify, Duolingo).</p> <p>Note</p> <p>A user might write and edit documents in LibreOffice or develop code in Visual Studio Code (application software) running on Ubuntu (system software), while the NVIDIA graphics driver (device driver) handles display and hardware acceleration, demonstrating how application software, the operating system, and device drivers work together to perform tasks efficiently.</p> <p>In addition to the distinction between system and application software, programs can also be categorized by how they are licensed and distributed, as proprietary or off-the-shelf software. Many private companies choose proprietary software such as Microsoft Office 365 because it offers professional customer support, cloud integration, and regular security updates. In contrast, public institutions and schools often adopt open-source alternatives like LibreOffice or Google Workspace (free edition) to reduce licensing costs while still providing students and staff with essential tools for document creation and collaboration. This demonstrates how the choice between proprietary and open-source software often depends on an organization's budget, technical needs, and support requirements.</p> <p>Exercise</p> <p>What are the advantages/disadvantages of proprietary and open-source software?</p>"},{"location":"Lecture_Notes/05_software/#operating-systems","title":"Operating Systems","text":"Graph of Operating System placement on computer usage. By     Golftheman \u2013     Own work,     CC BY-SA 3.0,     Link <p>Operating system (OS) is the core software that manages all hardware and software resources. The OS acts as a bridge between users, applications, and computer hardware. It translates high-level user commands into machine-level operations and manages system resources such as memory, processing time, and device communication to ensure efficient coordination across all components.</p> <p>Note</p> <p>Suppose a user is editing a document in Microsoft Word while listening to music on Spotify and downloading a file from the Internet. The operating system coordinates these simultaneous activities by dividing processor time among the programs, allocating memory to each, and managing input/output requests so that the music continues playing without interruptions and the document remains responsive.</p> <p>The kernel is the core part of an operating system. The kernel is the core component responsible for controlling the computer's hardware and managing system resources. It does not provide windows, menus, or buttons, and users do not interact with it directly. Instead, the kernel acts as a mediator between software and hardware, ensuring that programs use the CPU, memory, storage, and devices in a safe and orderly way.</p> <p>Note</p> <p>When an application needs to perform an action such as saving a file, accessing the internet, or displaying graphics, it cannot communicate with the hardware on its own. It sends a request to the operating system, which is handled by the kernel. The kernel checks whether the request is allowed, allocates the necessary resources, and communicates with the appropriate hardware through device drivers. This prevents programs from interfering with each other or damaging the system.</p> <p>The kernel is also responsible for multitasking. When multiple programs are running at the same time, the kernel rapidly switches the CPU between them, giving each program a small time slice. To the user, all programs appear to run simultaneously, but the kernel carefully controls this sharing to keep the system stable and responsive.</p> <p>An OS includes much more than the kernel. It also contains system libraries, background services, device drivers, and user interfaces. This is why different operating systems can be built around the same kernel. For example, Ubuntu uses the Linux kernel together with tools and interfaces designed for desktop and server use, while Android uses the same Linux kernel but combines it with a mobile-focused runtime and user interface. In this way, the kernel provides the foundation, and the OS builds a complete environment on top of it.</p>"},{"location":"Lecture_Notes/05_software/#additional-material","title":"Additional Material","text":"<ul> <li>Programming Basics: Statements &amp; Functions: Crash Course Computer Science #12</li> <li>Interpreters and Compilers (Bits and Bytes, Episode 6)</li> <li>Why You Shouldn't Nest Your Code</li> <li>Operating Systems: Crash Course Computer Science #18</li> <li>The real reason Boeing's new plane crashed twice</li> <li>Ariane 5 rocket launch explosion</li> <li>Programming myths that waste your time</li> </ul>"},{"location":"Lecture_Notes/06_network/","title":"06. Telecommunications and Computer Networks","text":"<p> Rahida Asadli, Ismayil Shahaliyev Nov 23, 2025 Jan 31, 2026</p>"},{"location":"Lecture_Notes/06_network/#telecommunications","title":"Telecommunications","text":"<p>Telecommunications at its core is the long-distance transmission of information using electronic or electromagnetic signals. Whenever two devices exchange information (e.g. a voice call, video stream, bank transaction) some form of telecommunications is functioning in the background. Telecommunications allows communication to happen without the sender and receiver being physically close. A phone call from one country to another, a university campus connecting its buildings, or a satellite sending weather data to Earth are all examples of telecommunications enabling interaction across space.</p> <p>Every telecommunication system follows the same basic structure: sender \u2192 channel (medium) \u2192 receiver. The sender is where information originates. For example, when you type a message on a smartphone, the phone acts as the sender by converting your text into a digital signal suitable for transmission. The signal then travels through the channel, which is the path between sender and receiver. This path may be a physical medium, such as a copper or fiber-optic cable, or a wireless medium using electromagnetic waves, as in Wi-Fi or mobile networks. Different channels have different characteristics, affecting speed, cost, and resistance to interference. At the receiver, the incoming signal is converted back into a form the user can understand, such as readable text on a phone screen. For communication to work, both sender and receiver must follow shared rules called protocols, which define how data is formatted, transmitted, and interpreted. Without protocols, devices would not be able to understand each other.</p>"},{"location":"Lecture_Notes/06_network/#glossary","title":"Glossary","text":"<p>Before starting our discussion, it is important to understand some of the technical terms used in telecommunications and networks. These terms appear frequently in networking and telecommunications, and knowing them makes it easier to understand more advanced concepts.</p> <p>Attenuation refers to the weakening or loss of a signal as it travels through a transmission medium. All signals gradually lose strength the farther they move, but the amount of loss depends on the cable type. Electrical signals traveling through copper wires, for example, lose strength much more quickly than light signals traveling through fiber-optic cables. If attenuation becomes too high, the receiver may get a distorted or unreadable signal, which is why long cables often require repeaters or amplifiers.</p> <p>Electromagnetic Interference (EMI) describes the effect that external electrical or magnetic sources have on a cable's signal. Nearby power lines, motors, electronic devices, and even fluorescent lights create electromagnetic fields that can disturb data transmission. Copper-based cables are especially vulnerable to EMI because they carry electrical signals. Fiber-optic cables, on the other hand, use light instead of electricity and therefore cannot be influenced by electromagnetic fields at all.</p> <p>Bandwidth is the maximum amount of data that a communication medium or network can carry per second. It is similar to the width of a water pipe: the wider the pipe, the more water can flow through it at once. In the same way, a cable or wireless link with higher bandwidth can carry more bits of data every second. Bandwidth is usually measured in Mbps (megabits per second) or Gbps (gigabits per second). Bandwidth is not about how fast each bit moves - because all electrical or light signals always travel extremely fast - but about how much data can move at the same time.</p> <p>Note</p> <p>Imagine two Internet connections: one with <code>10 Mbps</code> and another with <code>100 Mbps</code>. If you try to download a <code>100 MB</code> video file, the <code>100 Mbps</code> connection can transfer far more data per second, so the file will download much faster. In cables, the same logic applies. A twisted-pair cable with a bandwidth of <code>100 Mbps</code> per 100 meters can only carry a moderate amount of information at once. A coaxial cable can support around <code>500 Mbps</code>, so it can transfer several times more data in the same timeframe. Fiber-optic cable can carry gigabits of data every second, meaning it has an extremely wide \"data highway\" that can support huge amounts of information without slowing down.</p> <p>Throughput is the actual amount of data that successfully travels through the network per second. While bandwidth is the theoretical maximum capacity, throughput represents what you really experience in real conditions. Many factors can reduce throughput, such as interference, cable quality, distance, network congestion, or equipment limitations. Network congestion happens when more data is trying to pass through the network than the network can handle at that moment. Because the \"data highway\" is full, everything slows down.</p> <p>Note</p> <p>Suppose your Wi-Fi connection has a bandwidth of <code>300 Mbps</code>, but your laptop receives only <code>80 Mbps</code> during real usage. This difference comes from obstacles like walls, other Wi-Fi networks nearby, or older hardware. Although the network could support <code>300 Mbps</code> in ideal conditions, the throughput is only <code>80 Mbps</code> because that is the actual rate at which data is being delivered. Throughput is always equal to or lower than bandwidth. It can never be higher because you cannot exceed the maximum capacity of the medium. When people complain that their \"Internet is slow,\" they are not complaining about bandwidth itself, they are complaining about reduced throughput caused by delays, interference, or congestion.</p> <p>Signal type indicates the physical form of the transmitted data. Twisted-pair and coaxial cables both carry electrical signals, meaning electrons move through a copper conductor. Fiber-optic cables carry light signals, where small, rapid flashes of light encode the data. The physical difference between electrons and light explains why fiber performs better in speed, distance, and reliability, as well as why they are more expensive.</p> Property / Type Twisted Pair Coaxial Cable Fiber-Optic Cable Cost Low Medium (\u22482\u20133\u00d7 twisted pair) High Installation Easy Easy Difficult Attenuation High Moderate Very low Signal type Electrical Electrical Optical (light) Bandwidth ~1\u2013100 Mbps Hundreds of Mbps to Gbps Gbps to Tbps Distance ~100 m ~100\u2013500 m Kilometers EMI sensitivity High Low None <p>Latency is the time it takes for a piece of data to travel from one point to another. Even if the bandwidth is high, high latency can make communication feel slow. Local networks usually have low latency, while long-distance communication may have higher latency.</p> <p>Exercise</p> <p>Determine bandwidth and throughput of your device's connection.</p>"},{"location":"Lecture_Notes/06_network/#direction","title":"Direction","text":"<p>Communication systems can be classified by the direction in which data flows between sender and receiver. In practice, information may flow in only one direction, alternate between directions, or move in both directions at the same time. These differences affect how interactive a system feels and what it can be used for. The three basic types are simplex, half-duplex, and full-duplex communication.</p> <p>Simplex communication is one-way only. Data flows from sender to receiver, and the receiver cannot respond over the same channel. There is no interaction. This model is used when feedback is unnecessary and the goal is simply to distribute information. A television broadcast is a classic example: the station sends signals, and TVs only receive them. Digital billboards and public display screens work the same way - they show information but send nothing back.</p> <p>Half-duplex communication allows data to flow in both directions, but not simultaneously. A device can either send or receive at any given moment, and communication happens by taking turns. This is simpler and cheaper than full-duplex but slower and less natural. Walkie-talkies are the typical example: one person speaks while the other listens, then they switch. Older shared-cable Ethernet networks and many radio systems for taxis or emergency services also use half-duplex communication.</p> <p>Full-duplex communication allows simultaneous two-way data transfer. Both sides can send and receive at the same time, creating smooth and natural interaction. This requires more complex system design but provides the best user experience. Phone calls are a clear example: both people can speak and hear each other at once. Modern mobile networks, video calls, and Internet connections also rely on full-duplex communication, enabling fast, continuous exchange of data.</p>"},{"location":"Lecture_Notes/06_network/#time","title":"Time","text":"<p>Communication systems can also be classified based on the time relationship between the sender and the receiver. Timing determines whether both parties must be active at the same moment, and it influences how quickly information travels and how interactive the communication feels. When timing is strict and aligned, communication becomes immediate and real-time. When timing is flexible, systems allow delays and do not require participants to respond instantly. This distinction shapes the design of many modern technologies, from video calls to email platforms.</p> <p>Synchronous communication requires both endpoints to be active at the same time and to exchange data immediately. This model tightly couples the sender and receiver in time and often in execution flow. It assumes low latency, predictable availability, and sufficient resources on both sides. Synchronous communication is therefore appropriate when results are needed immediately or when later processing would be incorrect, such as real-time control systems, live audio/video streaming, or interactive remote procedure calls where the caller blocks until a response arrives.</p> <p>Asynchronous communication removes this temporal coupling. Asynchronous communication is not primarily about delayed human interaction. It is a core architectural choice used to handle latency, partial failure, load imbalance, and limited resources in scalable and resilient systems.</p> <p>The sender and receiver do not need to be active simultaneously, and the sender does not block waiting for the receiver to process the message. Instead, data is buffered, queued, or persisted until it can be handled. This model exists primarily to improve scalability, fault tolerance, and resource utilization in technical systems.</p> <p>Asynchronous communication is essential when workloads are sudden or unpredictable. For example, email servers process messages asynchronously so that incoming traffic spikes do not overload delivery or storage systems. Messages are queued and processed in batches, allowing throughput to be optimized independently of arrival time. Asynchronous execution is also critical for efficient use of computational resources. Long-running tasks such as data processing jobs, model training, video transcoding, or backup operations are executed asynchronously so they do not block user requests or real-time services. Task schedulers and job queues distribute work when CPU, memory, or GPU resources become available.</p>"},{"location":"Lecture_Notes/06_network/#range","title":"Range","text":"<p>Another important way to understand communication systems is by the physical range they cover. Distance determines how far signals must travel and what types of technologies or transmission media are required. A small-scale personal network inside a room operates very differently from massive global networks that span oceans and continents. As the distance grows, the complexity, infrastructure, cost, and technologies involved grow as well.</p> <p>Personal Area Network (PAN) covers only a very small area, usually within a few meters around a single person. PANs connect personal devices such as smartphones, smartwatches, earbuds, fitness trackers, and laptops. The purpose of a PAN is to allow an individual user's devices to communicate with each other seamlessly and wirelessly.</p> <p>Local Area Network (LAN) covers a limited geographic area such as a home, office, school building, or university campus. LANs are designed to connect multiple devices so they can share resources like printers, databases, internet access, and internal applications. LANs usually rely on Ethernet cables, Wi-Fi access points, switches, and routers to ensure reliable communication among devices within the same location.</p> <p>Note</p> <p>A typical example of a LAN is the network inside your home. Your router distributes internet access to your phone, laptop, smart TV, and gaming console. All these devices are part of the same LAN, which allows them to communicate with each other efficiently. Another example is an office network where dozens of computers connect to a central server, shared printers, and company databases. University labs, libraries, and academic buildings also rely on LANs to allow students and staff to access educational resources. LANs are known for high speed, low cost, and strong control since everything is located in a confined area.</p> <p>Metropolitan Area Network (MAN) spans a larger area than a LAN, typically a city or a large campus consisting of multiple buildings. MANs are used when organizations need to connect several LANs together across a broader space while maintaining high-speed communication.</p> <p>Wide Area Network (WAN) covers extremely large geographic areas, such as multiple cities, countries, or even entire continents. WANs are designed to support long-distance communication across thousands of kilometers. Because of the enormous distances involved, WANs use advanced technologies such as satellite links, undersea fiber-optic cables, high-speed backbone networks, and powerful routers capable of long-distance routing.</p> <p>Note</p> <p>The most significant example of a WAN is the Internet itself, which links billions of devices across the planet. When someone in Azerbaijan visits a website hosted in the United States, the data travels through a WAN that spans multiple networks, undersea cables, and data centers. International banking networks are also WANs because they allow ATMs and bank branches around the world to access the same financial systems. Global shipping companies, airlines, and multinational corporations rely on WANs to connect their offices across continents. No matter the scale, a WAN always involves long-distance communication that joins many smaller networks into one vast global system.</p>"},{"location":"Lecture_Notes/06_network/#computer-networks","title":"Computer Networks","text":"<p>A computer network is a collection of devices connected so they can exchange data. Devices communicate by sending small units of information across cables or wireless signals. To make this possible, each device must use specific hardware and addressing methods. The following terms describe the key building blocks of any network.</p>"},{"location":"Lecture_Notes/06_network/#glossary_1","title":"Glossary","text":"<p>MAC (Media Access Control) address is a unique, hardware-based identifier for network devices, like a serial number for your Wi-Fi or Ethernet card, used to identify them on a local network segment (like your home Wi-Fi) for data delivery, often shown as 12 hexadecimal digits (e.g. <code>00:1A:2B:3C:4D:5E</code>) and sometimes called a physical or burned-in address. It is crucial for network communication, ensuring data packets reach the correct device, and can be found in device settings or using command-line tools like <code>ipconfig /all</code> on Windows.</p> <p>IP (Internet Protocol) address is a logical address assigned to a device so it can communicate across different networks. Unlike the MAC address, an IP address can change depending on the network you connect to. Routers use IP addresses to determine the correct path for data across wider networks such as the internet.</p> <p>Hub is a simple device that connects multiple computers but does not analyze or direct data intelligently. When it receives data from one device, it copies that data to all connected devices. Hubs do not read addresses, which creates unnecessary traffic and makes them inefficient.</p> <p>Switch is a more advanced version of a hub. It learns the MAC addresses of connected devices and forwards data only to the specific device that should receive it. This reduces traffic and increases network performance. Switches operate inside a local network.</p> <p>Router connects different networks together. It reads IP addresses in the data and chooses the best route for packets to reach their destination. A home router connects your local network (your devices) to your internet service provider and then to the wider internet.</p> <p>Modem connects a local network to the physical infrastructure of the internet service provider (ISP). It modulates discrete digital data from a computer into a continuous physical signal suitable for transmission over the communication medium, and then converts the received continuous signal back into discrete digital data. This process is called modulation and demodulation. The modem does not route traffic, it provides access to the ISP network. In practice, many home devices combine modem, router, and switch functionality into a single unit, but these are logically distinct components.</p> <p>Frame is the unit of data used inside a local network when switches forward information. Frames carry MAC addresses. When data leaves the local network and moves through routers, it becomes packets that carry IP addresses.</p> <p>Packet is a small unit of data that travels across a network. Each packet contains the data being sent plus addressing information, which allows it to reach the correct destination even if packets travel different paths.</p> <p>Packet switching works by breaking data into small packets and sending them independently through the network. If a network sent data as one large continuous stream, the channel would be occupied until the entire transmission finished, blocking others. With packet switching, packets from many users are interleaved on the same links. This allows the network to stay busy and avoids wasting bandwidth when one sender is slow or idle. Packets can also take different paths through the network. If a link or router fails, packets are automatically rerouted through alternative paths. If some packets are lost or corrupted, only those packets are retransmitted, not the entire message. This makes the system robust against failures and congestion.</p> <p>Port in networking is a logical endpoint used by applications to separate different types of communication. For example, web traffic uses port <code>80</code> or <code>443</code>. Ports allow multiple services to run on the same device without mixing their data.</p> <p>Exercise</p> <p>Determine IP and MAC addresses in your device.</p>"},{"location":"Lecture_Notes/06_network/#network-topologies","title":"Network Topologies","text":"<p>Network topology describes the physical or logical arrangement of devices and connections in a network. It defines how nodes (computers, switches, routers) are interconnected and how data flows between them.</p> <p>Physical topology refers to the actual cables and hardware connections, when logical topology refers to how data moves regardless of the physical wiring. The choice of topology directly affects latency, bandwidth utilization, ease of expansion, and how failures propagate through the network.</p> <p>Common network topologies include bus, star, ring, mesh, and hybrid forms. Each topology represents a different trade-off between simplicity, cost, performance, and resilience. Early networks favored simple topologies due to hardware limitations, while modern networks often use hierarchical or hybrid topologies to support large scale and high availability.</p>      Network Topologies. By     NetworkTopologies.png:     Maksim / derivative work:     Malyszkz \u2013     Own work based on:     NetworkTopologies.png,     Public Domain,     Link"},{"location":"Lecture_Notes/06_network/#bus-topology","title":"Bus Topology","text":"<p>In a bus topology, all devices are connected to a single main cable, often called the backbone. Every device sends data onto this shared cable, and all other devices listen, but only the intended device processes the data. Because all communication uses the same line, too many devices can cause collisions and slow performance. If the main cable fails, the entire network stops functioning.</p> <p>Note</p> <p>Early office networks used a single coaxial cable running along the floor or ceiling, and all computers tapped into it. If someone accidentally damaged that cable, no computer could communicate.</p>"},{"location":"Lecture_Notes/06_network/#star-topology","title":"Star Topology","text":"<p>In a star topology, all devices connect to a central device, typically a switch or hub. Each device has its own cable to the central point. The central device controls all data flow and decides where data should go. If one cable fails, only that one device is affected; the rest of the network continues working. However, if the central switch fails, the entire network stops.</p> <p>Note</p> <p>Modern home and office networks use star topology. Every computer, printer, or access point connects individually to a central switch or router.</p>      Star Network. By Umapathy \u2013     Own work,     CC BY-SA 3.0,     Link"},{"location":"Lecture_Notes/06_network/#ring-topology","title":"Ring Topology","text":"<p>In a ring topology, each device connects to exactly two others, forming a closed loop. Data travels around the ring in one direction (or sometimes both directions), passing through each device until it reaches the destination. If one connection breaks, the entire loop can be disrupted, unless the network is designed with automatic rerouting.</p> <p>Note</p> <p>Some older MANs and early campus networks connected buildings in a circular loop. Data traveled from one building to the next until it reached the correct location.</p>"},{"location":"Lecture_Notes/06_network/#mesh-topology","title":"Mesh Topology","text":"<p>In a mesh topology, every device connects directly to several other devices. There are multiple paths for data to travel. If one link fails, the network automatically uses another path. This provides very high reliability and performance but requires more cables and is more expensive to build.</p> <p>Note</p> <p>Internet backbone providers use mesh topology. Major routers in different cities and countries connect through multiple redundant links so that if one line fails, global communication continues without interruption.</p> Topology Advantages Disadvantages Bus Simple to set up; requires less cable; inexpensive for small networks. Entire network stops if the main cable fails; performance degrades as devices increase; difficult to troubleshoot. Star Easy to manage; failure of one device does not affect others; easy to add or remove devices; good performance. If the central hub or switch fails, the whole network fails; requires more cabling than bus or ring. Ring Data flows in an organized, predictable sequence; no collisions; equal access for all devices. Failure of one device or link can break the entire ring; difficult to reconfigure or add devices. Mesh Highly reliable due to multiple paths; excellent fault tolerance; strong performance under heavy traffic. Expensive due to extensive cabling; complex installation and maintenance. Line Simple linear structure; easy to extend by adding devices at the ends. Failure of a middle device breaks communication beyond that point; limited scalability. Tree Easy to expand; supports hierarchical control; commonly used in large networks (e.g., campuses). Failure of the backbone affects entire branches; requires more cabling; more complex than a simple star."},{"location":"Lecture_Notes/06_network/#additional-material","title":"Additional Material","text":"<ul> <li>Computer Networks: Crash Course Computer Science #28</li> <li>MAC address explained</li> <li>Hub, Switch, &amp; Router Explained - What's the difference?</li> <li>Network Topologies (Star, Bus, Ring, Mesh, Ad hoc, Infrastructure, &amp; Wireless Mesh Topology)</li> <li>Network Ports Explained</li> <li>What is Ethernet?</li> </ul>"},{"location":"Lecture_Notes/07_internet/","title":"07. Internet and World Wide Web","text":"<p> Nilufar Ismayilova, Rumiyya Alili, Ismayil Shahaliyev Nov 21, 2025 Jan 31, 2026</p>"},{"location":"Lecture_Notes/07_internet/#internet","title":"Internet","text":"<p>The Internet is a global network of interconnected computer networks that allows devices around the world to communicate. It is not controlled by any single organization; instead, many independent networks - such as home networks, university networks, company networks, and government networks - connect to each other voluntarily. Together, these connections form a large, decentralized system that enables information to travel quickly and reliably between distant points.</p> <p>For communication to be possible across such a system, all connected devices must follow common rules. These rules are called network protocols. A protocol defines how data is sent, in what format, in what order, and what happens if errors occur. Without protocols, networks could be physically connected, but devices would not understand each other, making meaningful communication impossible.</p> <p>The Internet follows a hierarchical structure. Local networks connect to Internet Service Providers (ISPs), which then connect to larger regional networks and finally to high-speed backbone networks that carry data across countries and continents. When you open a website or send a message, your data travels through this chain of networks according to agreed-upon protocols until it reaches its destination.</p>"},{"location":"Lecture_Notes/07_internet/#internet-protocol-suite","title":"Internet Protocol Suite","text":"<p>The Internet Protocol Suite, commonly known as TCP/IP, is the foundational set of communication rules that defines how data is prepared, addressed, transmitted, and received across the Internet. It organizes network communication into layers, where each layer performs a specific part of the process. At the core of the suite is the Internet Protocol (IP), which assigns unique addresses to devices and routes data packets from the sender to the correct destination, even if they pass through many different networks. Above IP, the suite includes transport protocols that manage how data is broken into smaller pieces, how those pieces travel, and how they are reassembled when they reach the receiver. Together, these layers ensure that data moves smoothly across any combination of networks, making global communication possible.</p> <p>TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are the two main transport protocols used in the Internet Protocol Suite, and both serve different purposes in network communication. TCP provides reliable, ordered, and error-checked delivery of data. It establishes a connection between the sender and receiver before any data is transferred and ensures that every packet arrives correctly. If a packet is lost, TCP automatically resends it. This makes TCP suitable for activities where accuracy matters, such as loading websites, sending emails, or downloading files. UDP, on the other hand, is a faster but less reliable protocol. It sends packets without establishing a connection and does not check whether the receiver actually gets them. Lost packets are simply skipped, not retransmitted. Because it avoids extra overhead, UDP is used for real-time applications where speed is more important than perfect accuracy, such as video calls, online gaming, and live streaming.</p> <p>Note</p> <p>When you visit a website such as www.ada.edu.az, your computer sends a request that is divided into packets and addressed to the server's IP address. Routers along the path use this address to forward the packets across multiple networks until they reach the destination. Because web traffic uses TCP, the packets arrive reliably and in order. The server receives the complete request, processes it, and sends the response back using the same transport mechanism. If the same exchange were part of a real-time application, such as a video call or an online game, UDP could be used instead. In that case, packets would be sent without waiting for acknowledgments, favoring speed over perfect delivery.</p>"},{"location":"Lecture_Notes/07_internet/#ip-address","title":"IP Address","text":"<p>IP addressing is one of the core functions of the Internet Protocol Suite because it allows every device on a network to be uniquely identified. Without IP addresses, routers would have no way of knowing where data packets should go. An IP address functions much like a home address: it tells the network exactly where to deliver information and where it came from. The most common earlier version, IPv4, uses a 32-bit structure divided into four decimal numbers separated by dots, such as <code>192.168.1.4</code>. Each number represents eight bits, and together they uniquely identify a device on a network.</p> <p>As the number of Internet-connected devices grew rapidly, the IPv4 address space became insufficient. To solve this limitation, IPv6 was introduced. IPv6 uses 128 bits and is written as eight hexadecimal blocks separated by colons, for example <code>2001:db8:85a3::8a2e:370:7334</code>. This much larger address space allows an enormous number of unique addresses (each block has the range <code>0000-FFFF</code>, instead of <code>0-255</code>), removing the need for address scarcity workarounds.</p> <p>Exercise</p> <p>How many devices can be encoded with IPv4? What about IPv6?</p>"},{"location":"Lecture_Notes/07_internet/#domain-name-system","title":"Domain Name System","text":"<p>Domain Name System (DNS) is a global directory service that translates human-readable website names into numerical IP addresses that computers use to identify servers on the Internet. Without DNS, users would have to memorize long strings of numbers instead of simple domain names. It works automatically in the background, making it possible for browsers to locate and connect to the correct server whenever a user types a website name.</p> <p>DNS has a hierarchical structure made of several levels that help organize domain names from the highest level down to specific services. At the top is the root domain, represented by a dot (<code>.</code>), which is usually hidden from users but underlies all domain names. Below it are top-level domains (TLDs) such as <code>.com</code>,<code>.az</code>,<code>.org</code>, which classify domains by purpose or region. Under each TLD are second-level domains, which are the names registered by organizations or individuals, such as edu.az. These can be further divided into subdomains, which identify specific services or sections within an organization, for example ada.edu.az or library.ada.edu.az (subdomain of a subdomain).</p> <p>Note</p> <p>Imagine you want to visit www.ada.edu.az. Your computer does not know the IP address of this website, so it cannot contact the server directly. Instead, it asks DNS to look it up.</p> <ol> <li>You type: <code>ada.edu.az</code></li> <li>Your computer asks a DNS server: \"What is the IP address for this website?\"</li> <li>DNS checks its records or asks other DNS servers if needed.</li> <li>DNS replies with an IP address, for example: 104.21.45.120</li> <li>Your browser uses this IP to contact the ADA University server and load the webpage.</li> </ol> <p>So, DNS allows you to use easy names like <code>ada.edu.az</code> instead of memorizing long IP numbers. It works automatically in the background every time you visit any website.</p>"},{"location":"Lecture_Notes/07_internet/#routing","title":"Routing","text":"<p>Routing is the process by which routers determine the best path for data packets to travel across networks from source to destination. When you send data over the Internet, it does not travel in a straight line. Instead, it passes through multiple routers, each making decisions about where to forward the packet next. Routing ensures that data reaches its destination efficiently, even when there are many possible paths.</p> <p>Static Routing involves manually configuring the routes that a router will use. Network administrators enter specific rules that tell the router exactly where to send packets for particular destinations. Static routing is predictable and uses minimal router resources, but it does not adapt to network changes. If a link fails, the router cannot automatically find an alternative path. Static routing is typically used in small, stable networks where routes do not change frequently.</p> <p>Default Routing is a simplified form of routing where the router is configured to send all packets that do not match any specific route to a single next-hop router. This is useful in small networks or at the edge of larger networks, where there is only one path to reach the rest of the Internet. For example, a home router usually uses default routing, sending all external traffic to the ISP's router.</p> <p>Dynamic Routing uses protocols and algorithms to automatically discover and adjust routes based on current network conditions. Routers running dynamic routing protocols share information with each other about the state of the network, such as which links are active, which are congested, and which have failed. When a path becomes unavailable, dynamic routing automatically recalculates the best alternative route. This makes dynamic routing highly adaptable and scalable, which is why it is used in large networks and across the Internet backbone.</p> <p>Note</p> <p>Imagine you send an email from Baku to a friend in New York. Your data packet leaves your computer and reaches your home router, which uses default routing to forward it to your ISP. The ISP's routers use dynamic routing protocols to determine the best path across the Internet. As the packet travels through multiple routers in different countries, each one reads the destination IP address and decides where to send it next based on current network conditions. If one link is congested or down, dynamic routing ensures the packet takes an alternative path. Eventually, the packet reaches your friend's ISP, then their local router (using default routing), and finally their device. This entire journey happens in milliseconds because routing protocols work automatically and efficiently to move packets across the global Internet.</p>"},{"location":"Lecture_Notes/07_internet/#osi-model","title":"OSI Model","text":"<p>OSI (Open Systems Interconnection) model explains how networking systems communicate across seven layers. Each layer performs a specific role and interacts only with the layers above and below it.</p> # Layer Function Example 7 Application Provides services directly to user applications; defines how browsers, email clients, and apps request network services; uses protocols like HTTP, DNS, FTP, and SMTP. When you type www.ada.edu.az into your browser, the Application Layer uses HTTP to request the webpage and DNS to resolve the domain name. 6 Presentation Translates data formats between applications and the network; handles text encoding (ASCII/Unicode), compresses files, and performs encryption/decryption such as TLS/SSL. When the website uses HTTPS, this layer encrypts your data (TLS) so attackers cannot read login information or credit card details. 5 Session Establishes, manages, and terminates communication sessions between computers; keeps track of ongoing dialogs so data streams don\u2019t mix. When watching a Zoom lecture, the Session Layer maintains a stable session so your audio/video stream stays connected even if packets arrive out of order. 4 Transport Ensures end-to-end communication; provides reliable delivery (TCP), flow control, retransmission of lost packets, or fast low-overhead delivery (UDP). When loading a website, TCP guarantees every piece of the webpage arrives in order. For online gaming, UDP sends rapid updates without waiting for lost packets. 3 Network Handles routing of packets between networks; assigns logical IP addresses and finds the best path through routers; uses IP. Routers along the Internet path use the destination IP address to forward your webpage request toward the ADA University server. 2 Data Link Creates frames for local network communication; uses MAC addresses; handles error detection on the link (Ethernet, Wi-Fi frames). Your laptop sends the request to your Wi-Fi router using the router\u2019s MAC address, and the router sends frames to your ISP. 1 Physical Transmits raw bits (0s and 1s) through cables, fiber optics, or wireless signals; defines voltages, frequencies, and modulation. Wi-Fi converts frames into radio waves, or Ethernet converts them into electrical pulses that travel through a cable. <p>Note</p> <p>When you open <code>www.ada.edu.az</code>, the process begins at the Application Layer, where the browser prepares an HTTP request. The Presentation Layer applies encryption if the connection uses HTTPS. The Session Layer sets up and maintains the communication session. The Transport Layer (TCP) breaks the request into small segments and ensures they are delivered reliably. The Network Layer (IP) attaches source and destination IP addresses so routers can forward the data across different networks. The Data Link Layer formats the data into frames addressed to your local router's MAC address. Finally, the Physical Layer converts the frames into radio signals or electrical pulses that travel across Wi-Fi or Ethernet. On the server's side, the same layers work in reverse, eventually allowing your browser to receive the website's content and display it on your screen.</p>"},{"location":"Lecture_Notes/07_internet/#intranet-and-extranet","title":"Intranet and Extranet","text":"<p>An intranet is a private network used within an organization. It provides internal services such as employee portals, shared files, calendars, and communication tools. Because an intranet is restricted to internal users, it offers better security and control. Only employees or authorized personnel can access it, usually through login credentials or the company's internal Wi-Fi.</p> <p>An extranet extends the intranet by allowing limited access to external users such as business partners, suppliers, contractors, or students. Organizations use extranets to collaborate with outsiders while still protecting sensitive internal information. Access to an extranet is controlled through accounts, permissions, or Virtual Private Network (VPN) connections.</p> <p>Note</p> <p>An intranet example is a company's internal management system. Only employees connected to the company's network, or logged in with corporate credentials, can access internal resources such as financial reports, staff schedules, internal announcements, and confidential documents. Customers and the general public cannot access these resources because the intranet is strictly internal and protected. An extranet example is when the same company allows selected business partners or contractors to access specific shared resources through a secure login. These external users might view project timelines, shared technical documents, or order statuses, but they cannot access the company's full internal systems.</p>"},{"location":"Lecture_Notes/07_internet/#world-wide-web","title":"World Wide Web","text":"<p>The World Wide Web is a global information system that allows users to access and interact with digital content through web browsers. It consists of millions of interconnected web pages stored on web servers and accessible through the Internet. Unlike the Internet, which is the physical network of computers and cables, the Web is a service that uses that network to deliver information. Web pages are linked using hyperlinks, which allow users to jump from one page to another, creating a web-like structure of information. Each web page is written using HTML and can include text, images, videos, animations, or interactive elements. Browsers such as Chrome, Firefox, and Safari request pages from servers and render them into a visual format the user can understand.</p> <p>Note</p> <p>Imagine you want to check your course grades on the ADA University website. You open your browser and type <code>www.ada.edu.az</code>. Your browser contacts the ADA University web server and requests the HTML file that contains the homepage. The server sends back this page along with images, styles, and scripts. Your browser then renders the information into a visually formatted website. All of this interaction, requesting pages, receiving files, navigating through hyperlinks, is what makes up the World Wide Web.</p> <p>Hypertext Transfer Protocol (HTTP) is the foundational communication protocol used by the Web. It defines how web browsers send requests and how web servers respond. When you visit a website, your browser sends an HTTP request asking the server for specific resources such as HTML pages, images, or scripts. The server then sends an HTTP response containing the requested content, along with a status code (like 200 OK or 404 Not Found) that explains whether the request was successful.</p> <p>Modern websites usually use HTTPS instead of HTTP. HTTPS (S stands for \"secure\") adds an encryption layer through Transport Layer Security (TLS), which protects the data being sent between the browser and the server. This prevents attackers from reading or modifying information, making HTTPS essential for activities like logging into accounts, entering credit card information, or sending personal data.</p> <p>Note</p> <p>Suppose you log into your ADA student account. When you type your username and password and click \"Sign In\", your browser sends an HTTPS request to the ADA authentication server. The server processes your credentials and sends back an HTTPS response that either grants access or shows an error. Because the connection uses HTTPS, your login information is encrypted, preventing anyone on the network, such as someone on the same Wi-Fi, from reading your password. Without HTTPS, the login request could be intercepted and viewed in plain text.</p> <p>Uniform Resource Locator (URL) is the full address that identifies exactly where a resource is located on the Web. It tells the browser how to access the resource (protocol), where to find it (domain name), and which specific file or page to retrieve (path). A typical URL may look like https://www.ada.edu.az/en/search?query=SITE which can be broken down into clear parts:</p> <ul> <li><code>https</code> - the protocol indicating secure communication</li> <li><code>www.ada.edu.az</code>- the domain name of the server</li> <li><code>/en/search</code> - the path to the specific folder and file on the website</li> <li><code>?query=SITE</code> - a query parameter used to request a specific item or version of a page</li> </ul>"},{"location":"Lecture_Notes/07_internet/#markup-languages","title":"Markup Languages","text":"<p>Markup languages are systems for annotating text so that computers can understand how the information should be structured, displayed, or processed. They do not describe the meaning of the content but rather its organization or formatting. The most widely used markup language is HTML (HyperText Markup Language), which forms the structure of web pages on the World Wide Web. HTML uses tags such as <code>&lt;h1&gt;</code>, <code>&lt;p&gt;</code>, and <code>&lt;img&gt;</code> to define headings, paragraphs, and images. Browsers read these tags and convert them into the visual layouts that users see.</p> <p>Another well-known markup language is XML (Extensible Markup Language). Unlike HTML, XML is not about displaying information but about storing and transporting data in a structured, readable way. XML allows users to create custom tags that describe data, such as &lt;student&gt; or &lt;course&gt;, making it useful for data exchange between systems, configuration files, and web services.</p> <p>Modern web applications often use JSON (JavaScript Object Notation) alongside or instead of XML. JSON is a lightweight data format structured as key-value pairs, making it easy for computers to parse and for humans to read. It is widely used in APIs, web applications, and cloud services because it is simpler and faster than XML.</p> <p>Note</p> <p>A university website uses HTML to structure all its pages. For instance, the homepage might use <code>&lt;h1&gt;Welcome to ADA University&lt;/h1&gt;</code> for the main title, <code>&lt;p&gt;</code> for text paragraphs, and <code>&lt;img&gt;</code> tags to insert images. Everything the browser displays visually is built from HTML tags.</p> <p>At the same time, the website's backend might use JSON to exchange data with the server. For example, when a student checks their grades, the system might send back data like:</p> <pre><code>{\n  \"student\": \"Badambura\",\n  \"course\": \"SITE1101\",\n  \"grade\": \"A\"\n}\n</code></pre> <p>This data is not meant to be shown directly to the user but allows applications to process and update information quickly. On the other hand, a library database may store book information in XML, such as:</p> <pre><code>&lt;book&gt;\n  &lt;title&gt;Computer Networks&lt;/title&gt;\n  &lt;author&gt;Andrew Tanenbaum&lt;/author&gt;\n  &lt;year&gt;2020&lt;/year&gt;\n&lt;/book&gt;\n</code></pre>"},{"location":"Lecture_Notes/07_internet/#cloud-computing","title":"Cloud Computing","text":"<p>Cloud computing is a model of delivering computing resources, such as storage, servers, networks, databases, and software, over the Internet instead of relying on local hardware. Instead of buying and maintaining physical equipment, users access shared resources from cloud providers on demand. This makes computing more flexible, scalable, and cost-efficient. Organizations can increase or decrease their computing power instantly, paying only for what they use, which is especially useful for businesses with changing workloads or seasonal demand.</p> <p>Cloud services are often categorized into three main layers. Infrastructure as a Service (IaaS) provides virtual machines, storage, and networking. It is the closest to owning physical hardware but without the responsibility of maintaining it. Developers or IT teams use IaaS when they need control over operating systems and applications. Platform as a Service (PaaS) offers a complete environment for building and deploying applications. It hides server and system management so developers can focus solely on writing code. Software as a Service (SaaS) delivers ready-to-use applications that run entirely online. Users do not install anything; they simply log in through a browser.</p> <p>Note</p> <p>Imagine ADA University hosting its learning platform. Instead of buying physical servers, ADA could use Amazon Web Services (AWS). The university might run its website on IaaS, using AWS virtual machines to host pages and store data. Developers working on ADA's student mobile app could use PaaS, such as Google App Engine, to build and test new features without setting up their own servers. Students and staff would use SaaS tools like Microsoft Outlook or OneDrive to access email, documents, and files. If ADA wants higher security for sensitive student information, it might store that data in a private cloud while using the public cloud for the main website. This combination is an example of hybrid cloud computing.</p>"},{"location":"Lecture_Notes/07_internet/#centralized-decentralized-distributed-systems","title":"Centralized, Decentralized, Distributed Systems","text":"<p>A centralized system relies on a single central authority or server. All users and components depend on this central point for access and coordination. This design is simple to manage but fragile: if the central server fails or is compromised, the entire system may stop working.</p>      Graphical comparison of centralized (A) and decentralized (B) systems. By Kes47 (?) - Based on File:Decentralization.jpg, by Adam Aladdin, CC BY-SA 3.0, Link <p>A decentralized system has multiple authoritative nodes instead of one central controller. Control and data are spread across several independent points, each of which can operate on its own. If one node fails, others can continue functioning. The Internet itself is decentralized at the network level: no single organization controls it, and many independent networks cooperate to keep it running.</p> <p>A distributed system goes a step further by splitting computation, storage, and tasks across many nodes that work together as a single system. These nodes coordinate and communicate to achieve a common goal, often appearing as one system to the user. Cloud platforms, distributed databases, and content delivery networks are typical examples. Distributed systems focus on scalability, performance, and fault tolerance rather than on who controls the system.</p> <p>In short, centralized means one control point, decentralized means multiple independent control points, and distributed means many coordinated components working together. These concepts overlap in practice but describe different design goals and trade-offs.</p>"},{"location":"Lecture_Notes/07_internet/#additional-material","title":"Additional Material","text":"<ul> <li>What is Internet?</li> <li>The Internet: Crash Course Computer Science #29</li> <li>Public vs Private IP Address</li> <li>How a DNS Server (Domain Name System) works</li> <li>TCP vs UDP Comparison</li> <li>Network Layers Model (Networking Basics) - Computerphile</li> <li>VPN (Virtual Private Network) Explained</li> <li>The World Wide Web: Crash Course Computer Science #30</li> <li>What is the world wide web? - Twila Camp</li> <li>How trees secretly talk to each other - BBC World Service</li> <li>SSL, TLS, HTTP, HTTPS Explained</li> <li>HTTP Crash Course &amp; Exploration</li> <li>HTTP 1 vs HTTP 2 vs HTTP 3!</li> <li>DHCP Explained - Dynamic Host Configuration Protocol</li> <li>Cloud Computing Explained</li> <li>How to Setup VS Code for Web Development (2025) HTML, CSS, JavaScript + Live Server</li> <li>Every File Format Explained in 16 Minutes</li> </ul>"},{"location":"Lecture_Notes/08_database/","title":"08. Databases &amp; Database Management Systems","text":"<p> Rahida Asadli, Rumiyya Alili, Ismayil Shahaliyev Dec 1, 2025 Jan 31, 2026</p> <p>In the past, many systems stored information in simple text files. For example, a file named <code>students.txt</code> could have lines like:</p> <pre><code>1201, R\u0259fail, IT, 3.5\n1202, Nadal, CS, 3.9\n</code></pre> <p>Each line is one student. To use this information in a program, the programmer has to open the file, read each line, and split it into parts (ID, name, major, GPA). That \"splitting into parts\" is what we call parsing: taking one long string like <code>1201, R\u0259fail, IT, 3.5</code> and turning it into separate values: <code>1201</code>, <code>R\u0259fail</code>, <code>IT</code> , and <code>3.5</code>.</p> <p>If the university also has courses, grades, payments, dorms, and so on, there may be many different text files: <code>students.txt</code>, <code>courses.txt</code>, <code>grades.txt</code>, etc. Every application (mobile app, website, internal admin tool) must know how each file is structured and must write its own code to read, search, and update those files.</p> <p>This creates several problems (Exercise. Which problems?):</p> <ul> <li>If a value changes (for example, a department name), it may appear in many files. Someone has to open every file and change it everywhere. It is very easy to miss one place and end up with inconsistent data.</li> <li>To find something, the program usually has to read the whole file line by line, which becomes slow as the file grows.</li> <li>If two users or two programs try to edit the same file at the same time, the file can become corrupted, because there is nothing to coordinate their changes.</li> <li>If the computer crashes in the middle of writing to a file, part of the data might be lost or left in a broken state.</li> </ul> <p>To solve these problems, we use the database approach.</p>"},{"location":"Lecture_Notes/08_database/#database-approach","title":"Database Approach","text":"<p>Database is an organized collection of data, and a Database Management System (DBMS) is the software that manages this data for us. You can think of the DBMS as a very smart program whose only job is to store, protect, and provide access to data in a safe and efficient way.</p> <p>Instead of many separate text files, the DBMS may store data in tables with rows and columns. You describe to the DBMS what kind of data you want to store (for example: a table Students with columns StudentID, Name, Major, GPA), and then the DBMS takes care of: Storing the data on disk in an efficient way. Checking rules, such as \"every student must have a unique ID\". Allowing many users/programs to read and update the data at the same time without corrupting it. Recovering from crashes so your data is not lost.</p> <p>The big advantage is that applications no longer need to worry about reading raw files or parsing lines manually. They simply ask the DBMS: \"Give me all students from the IT major with GPA above 3.0\", and the DBMS finds and returns the result.</p> <p>Because the DBMS hides all the low-level details of how data is stored on disk, we can change the physical storage (for example, move to a faster disk, change indexes) without changing the applications that use the data. Applications only see the logical structure (tables and columns) while the DBMS handles the physical (e.g. hardware-level) interaction.</p> <p>In short: The traditional file-based approach = many text files + lots of custom code + high risk of inconsistency and corruption. The database approach = one central DBMS (just software) that stores data in tables, enforces rules, supports many users, and provides a simple and safe way to work with data.</p>"},{"location":"Lecture_Notes/08_database/#database-indexing","title":"Database Indexing","text":"<p>Note</p> <p>Suppose a database table contains 10,000 records, each identified by a unique numeric ID, and an index exists on this ID column. A query asks for the record with ID 7421. Without an index, the database would need to scan the table sequentially, potentially examining all 10,000 rows. With the index, the database starts near the middle of the index and compares against a value such as 5000. Since 7421 &gt; 5000, it ignores the lower half. It then compares against 7500, then 7000, and continues narrowing the range. After only a small number of comparisons, the exact index entry is found, and the database follows its pointer directly to the correct row.</p>      Binary search. By Mazen Embaby - Own work, CC BY-SA 4.0, Link <p>When a database table contains thousands or millions of records, searching for specific information can become very slow if the system has to read every single row. This is where database indexing becomes essential. A database index is a data structure that improves the speed of data retrieval operations on a database table. Think of it like the index at the back of a textbook: instead of reading every page to find information about Napoleon, you look up \"Napoleon\" in the index, which tells you exactly which pages to read. Similarly, a database index allows the system to quickly locate data without scanning every row.</p> <p>When you create an index on a table, the database manager requires you to specify an index key field, the column on which the index will be based. For example, if you frequently search for students by their Student ID, you would create an index on the Student ID column. The database then performs these steps:</p> <ul> <li>Reading and Extracting. The DBMS reads each record in the table and extracts the value from the key field along with a pointer that indicates where that record is physically stored.</li> <li>Sorting. These values are organized in sorted order as part of the index structure.</li> <li>Creating a Search Structure. The index is typically organized as a tree structure. This allows the database to search very efficiently. Instead of checking every entry, it uses a divide and conquer approach: it starts at the top of the tree and narrows down the search by following branches, similar to how you would search for a word in a dictionary by opening it roughly in the middle and then deciding whether to look in the first half or second half.</li> </ul> <p>In practice, databases do not rely on the simple binary search described in the example above. Instead, they use structures such as B-trees (or B+ trees) which generalize the same divide-and-conquer principle while being optimized for disk access and large datasets.</p> <p>While indexes dramatically speed up searches, they come with costs. Indexes require additional disk space to store the sorted key values and pointers. When you insert, update, or delete records, the index must also be updated, which takes extra time. Indexes need to be maintained and occasionally rebuilt as the database grows. For this reason, indexes are typically created on columns that are frequently used in search queries or for sorting, such as student IDs, email addresses, or dates.</p>"},{"location":"Lecture_Notes/08_database/#data-models","title":"Data Models","text":"<p>A database is never built \"in the abstract\". A database always models something that exists in the real world. It means that every important object and every important interaction in those processes should be representable as data in the database.</p> <p>Entity-Relationship (ER) diagram is a graphical representation of the data model. An ER diagram uses a small set of visual symbols to represent the structure of data. Each symbol corresponds to a key concept: entities, attributes, and relationships. ER diagram is the blueprint from which the entire database schema is built.</p> <p>Entity is something that exists independently and can be uniquely identified. It is an abstraction that captures one distinguishable concept in the real world. The important point is that each entity must be capable of having data stored about it, and it can be uniquely distinguished from all other entities. Entities behave like nouns in language. Examples: Employee, Computer, Song, Book, Department.</p> <p>Attributes describe the properties of an entity. For example, Student entity may have attributes: Name, Surname, GPA, Admission Year. Some attributes have special roles like, primary key attributes, uniquely identifying each instance.</p> <p>Relationship shows how two or more entities are connected. Relationships behave like verbs linking nouns (entities). For example, Employee works in Department, Artist performs Song.</p> <p>Each relationship has a cardinality (<code>1:1</code>, <code>1:N</code>, or <code>M:N</code>) that determines how the relationship will appear in the database schema. For example, one employee works in one department (<code>1:1</code>), one department has many employees (<code>1:N</code>), a student takes many courses and a course has many students (<code>M:N</code>).</p> <p>A data model can be the collection of all these entities, their attributes, and their relationships. These elements determine how tables will be constructed in the relational model. This data model is more than just storage: if the business rule says \"a student cannot enroll in the same course twice in the same semester\", that rule should be reflected in the model.</p> <p>Exercise</p> <p>Provide an example of good and bad data models.</p>"},{"location":"Lecture_Notes/08_database/#relational-database-model","title":"Relational Database Model","text":"<p>Relational database model is the most widely used way of organizing data today. In the relational model, all information is stored in tables, similar to spreadsheets. Each table has rows, where each row represents one item (for example, one student), and columns, where each column represents something we want to store about that item (such as the student's name or GPA). Every table must have a special column called a primary key, which uniquely identifies each row. This prevents duplicate data and helps the database keep everything organized. When one table needs to refer to a row in another table, it stores the primary key of that row. This stored value is called a foreign key, and it creates a link between tables.</p> <p>Exercise</p> <p>How can an ER diagram be represented with relational database model?</p> <p>Exercise</p> <p>Take two tables and provide examples for primary and foreign keys.</p> <p>Before relational databases existed, computers used different models to store data. One of these early models was the hierarchical model, which stored data in a tree-like structure. Imagine a company: at the top is the company itself, underneath are departments, and underneath each department are employees. Data could only be accessed by following this fixed path. This made the system rigid: if a programmer wanted to view employees grouped in a different way \u2013 for example, sorted by project instead of department \u2013 it became a challenging task. Changing the structure of the tree often broke the programs that depended on it.</p>"},{"location":"Lecture_Notes/08_database/#structured-query-language-sql","title":"Structured Query Language (SQL)","text":"<p>When comparing different data modeling options, the relational model remains dominant because it provides a balanced combination of structure, accuracy, and flexibility. Its clear table format makes the data easy to understand. Its rules, like primary keys and foreign keys, help prevent mistakes. And because relational databases were designed to be independent of physical storage, programs do not break when the database engine changes how it stores data internally. SQL makes querying relational data straightforward and powerful. For these reasons, relational databases are used in banks, universities, hospitals, e-commerce websites, government systems, and almost every field where consistent and reliable data is essential.</p> <p>SQL is a simple query language used to communicate with the relational database. The name comes from its original form, SEQUEL (Structured English Query Language), which was developed at IBM in the 1970s. The language was designed to work with the relational model proposed by Edgar F. Codd, whose work showed that data could be represented and queried using mathematical relations rather than low-level file structures. The name was later shortened to SQL for trademark reasons, but the pronunciation \"sequel\" remained common.</p> <p>Strictly speaking, SQL is not a single language, but a family of languages defined by international standards. Organizations such as ANSI and ISO publish SQL standards that specify core syntax and behavior. However, real database systems do not implement the standard exactly. Instead, each DBMS provides its own SQL dialect, extending or modifying the standard. Common SQL implementations include Oracle, MySQL, PostgreSQL, Microsoft SQL Server, and SQLite. While these systems share a large common subset of SQL, they differ in features, syntax extensions, and behavior.<sup>1</sup></p> <p>Instead of describing how data should be accessed, SQL allows users to specify what data they want, and the database system determines the most efficient way to retrieve it. The following query will retrieve two columns from the table students.</p> <pre><code>SELECT id, name FROM students\n</code></pre> <p>A new row to the table can be added with the following query:</p> <pre><code>INSERT INTO students (id, name, gpa) VALUES (1203, 'Badambura', 2.0)\n</code></pre> <p>Or deletion can be achieved based on a certain condition:</p> <pre><code>DELETE FROM students WHERE id=1203\n</code></pre>"},{"location":"Lecture_Notes/08_database/#database-normalization","title":"Database Normalization","text":"<p>Because SQL works directly with table names and column names, the structure of the data model becomes extremely important. SQL queries depend completely on how the data model is designed. A bad data model may allow duplication of information, contradictions, and anomalies when inserting, updating, or deleting data. To avoid bad data modeling, Edgar F. Codd and his colleagues at IBM developed a whole theory, known as normalization.</p> <p>Normalization gives us formal rules for structuring tables so that each fact is stored only once, dependencies are clear, and common update problems are avoided. Normalization helps ensure the schema is correct before SQL is written. A normalized schema reduces repeated data, avoids confusing table designs, keeps SQL queries stable for many years and makes it easier to maintain and extend the system. In practice, when you design databases in your professional career, you will rely on these normalization rules to evaluate and refine your schemas.</p> <p>Tip</p> <p>It is highly suggested for you to take at least a brief look at different normal forms, starting from the first normal form (1NF).</p> <p>A bad data model becomes a long-term problem. It forces teams to constantly rewrite SQL queries, fix broken reports, and update multiple systems when the structure changes. A good data model, built with normalization in mind, protects the entire system. It keeps SQL programs reliable, keeps data consistent, and makes the database easier to manage as the organization grows. This is why investing time in designing the data model properly at the beginning is one of the most important steps in database development.</p>"},{"location":"Lecture_Notes/08_database/#data-manipulation","title":"Data Manipulation","text":"<p>Relational algebra is the mathematical foundation of how relational databases operate. Although we eventually write SQL queries and may not write algebra expressions directly, SQL is built on top of these relational algebra ideas (Edgar F. Codd was a mathematician in the end. Check out his infamous paper). The three most important operations in relational algebra are selection, projection and the idea of join. These operations describe how we retrieve information from tables, how we combine tables.</p> <p>Selection is the operation that chooses certain rows from a table based on a condition. For example, if we have a students table and want to find only the students from SITE, the database can perform a selection. In algebra, this is written as . In SQL, the same idea appears in the WHERE clause:</p> <pre><code>SELECT * FROM students WHERE school = 'SITE'\n</code></pre> <p>Selection does not change the columns of the table (not to confuse it with the SELECT keyword!). It only filters out the rows that do not match the rule. This operation corresponds to everyday questions such as \"Which employees work in the Finance department?\" or \"Which orders were created today?\"</p> <p>Projection is the operation that chooses certain columns from a table and ignores the others. If we take the Students table and want only the name and gpa, projection removes the unnecessary columns. Similar to selection's , in relational algebra, projection is denoted by the Greek letter \u03c0. In SQL, this appears in the SELECT keyword:</p> <pre><code>SELECT name, gpa FROM students;\n</code></pre> <p>Projection helps keep results focused and avoids returning more data than necessary. It supports questions like \"Show me only the names of students,\" \"List only the prices of products,\" or \"Give me the course codes without any other details.\"</p> <p>Joining is one of the most important relational algebra operations because it allows us to combine information stored in different tables. Since relational databases split data into multiple simple tables, joining them back together is essential for answering real questions. For example, suppose we have a students table and a separate enrollments table. If we want to know which students are taking a particular course, the database joins the two tables by matching rows that share the same student_id. In relational algebra, this is expressed using the join operator \u22c8 together with a join condition. In SQL, the equivalent is a JOIN statement:</p> <pre><code>SELECT students.name, enrollments.course_id FROM students JOIN enrollments USING (student_id);\n</code></pre> <p>Joining allows us to answer questions that cannot be answered using a single table alone, such as \"Which students are in this course?\", \"Which customer placed this order?\", or \"What flights does this passenger have reservations for?\"</p>"},{"location":"Lecture_Notes/08_database/#sql-rdbms-vs-nosql-nrdbms","title":"SQL (RDBMS) vs NoSQL (NRDBMS)","text":"<p>When choosing how to store data, modern systems usually rely on two major families of databases: SQL databases, also called relational database management systems (RDBMS), and NoSQL databases, also called non-relational database management systems (NRDBMS). Although both store and retrieve data, they are designed in different ways and are suited to different types of problems.</p> <p>SQL databases organize data into structured tables made of rows and columns, following the relational database model. The database enforces strict rules such as unique identifiers, relationships between tables, and integrity constraints. This strong structure makes SQL databases reliable and predictable, which is why they are commonly used in systems where accuracy and correctness are essential, such as banking systems, university registration systems, airline reservations, and other applications where data relationships must always be maintained.</p> <p>NoSQL databases take a more flexible approach. Instead of fixed tables, they store data using formats such as documents, key-value pairs, graphs, or wide columns. This flexibility allows the database to adapt easily when the structure of the data changes or when very large amounts of data must be stored and accessed quickly. As a result, NoSQL databases are often used in social networks, messaging platforms, recommendation systems, and applications that handle large volumes of unstructured or rapidly changing data.</p> <p>The key difference between SQL and NoSQL lies in their priorities. SQL databases emphasize structure, well-defined relationships, and correctness. NoSQL databases emphasize flexibility, scalability, and performance at large scale. For example, a bank would typically use a SQL database because financial data must remain accurate and consistent. In contrast, a chat application or social media platform may prefer a NoSQL database because it must handle millions of messages or posts per second and adapt quickly to changing data formats.</p> <p>In practice, many modern systems use both approaches together: SQL databases for core, structured data that must remain reliable, and NoSQL databases for components that require high scalability and flexibility, such as logs, messages, user activity feeds, and analytics.</p>"},{"location":"Lecture_Notes/08_database/#database-transactions","title":"Database Transactions","text":"<p>A transaction moves the database from one consistent state (State A) to another consistent state (State B) by executing a sequence of operations (Operation 1, Operation 2, Operation 3, etc.). If all operations succeed, the transaction is committed. Commit means that the database permanently saves all changes made during the transaction. Once committed, the changes become part of the database and cannot be automatically undone. If any operation fails (due to an error, constraint violation, or system crash), the transaction is rolled back. Rolling back means the database discards all changes made during the transaction and returns to the previous consistent state. It is as if the transaction never happened. This commit/rollback mechanism is what makes transactions reliable and is enforced by the ACID properties described below.</p> <p>Note</p> <p>Consider withdrawing money from an ATM.</p> <ol> <li>Your balance is 100 AZN.</li> <li>ATM checks if cash is available.</li> <li>ATM dispenses 10 AZN cash.</li> <li>System deducts 10 AZN from your balance.</li> <li>Your balance is now 90 AZN.</li> </ol> <p>If all operations succeed \u2192 COMMIT \u2192 Your balance permanently becomes 90 AZN. If Operation 2 fails (ATM runs out of cash) \u2192 ROLLBACK \u2192 No money is dispensed AND your balance stays 100 AZN. The database ensures you are not charged for money you did not receive. </p>"},{"location":"Lecture_Notes/08_database/#acid-properties","title":"ACID Properties","text":"<p>ACID describes the four rules a transaction must follow for the database to stay correct.</p> <p>A for Atomicity. A transaction must be completed fully, or not at all.</p> <p>Example</p> <p>You transfer 20 AZN from your Bank Account A to Account B. Two steps occur: 1) Subtract 20 AZN from A, 2) Add 20 AZN to B. If step 2) fails (network crash), step 1) must be undone. Atomicity ensures money does not \"disappear\".</p> <p>C for Consistency. The database must move from one valid state to another valid state</p> <p>Example</p> <p>If a rule says \"GPA must be between 0.0 and 4.0\", the database should never allow a transaction that sets GPA to 7.0.</p> <p>I for Isolation. Even if many users are working at the same time, each transaction should behave as if it is running alone. It is known as concurrency control</p> <p>Example</p> <p>Two students should not be allowed to register for the last available seat in a class at the same time.</p> <p>D for Durability. Once a transaction is completed, its result must stay, even if the computer crashes</p> <p>Example</p> <p>If you successfully buy a ticket and the system confirms it, the reservation should not disappear because of a power outage one second later.</p> <p>Traditional relational database systems were designed to manage structured data stored on a single logical database, often running on one machine or a tightly controlled cluster. In this setting, the primary challenge is to ensure that concurrent transactions behave correctly. This led to the ACID properties, which define what it means for a transaction to be reliable. ACID focuses on correctness: transactions should either fully succeed or fully fail, preserve database rules, remain isolated from one another, and survive system crashes. These guarantees are practical and achievable when the system operates under centralized control and reliable coordination.</p> <p>In contrast, many NoSQL systems were designed for large-scale, distributed environments spread across multiple machines and data centers. In such systems, network delays and failures are unavoidable, and it is not always possible to maintain strong coordination between all nodes. For this reason, the central concern shifts from transaction correctness to system behavior under network partitions. This is captured by the CAP theorem described below.</p>"},{"location":"Lecture_Notes/08_database/#cap-theorem","title":"CAP Theorem","text":"<p>CAP theorem states that a distributed data store cannot simultaneously guarantee Consistency, Availability, and Partition tolerance. For example, when a network partition occurs, the system must choose between remaining available or maintaining strict consistency. In distributed systems, a system cannot guarantee all three.</p> <p>C for Consistency. Everyone sees the same data</p> <p>Example</p> <p>If you \"like\" a photo, your friend should immediately see the updated like count.</p> <p>A for Availability. The system always responds.</p> <p>Example</p> <p>Even if the system is busy, Instagram must show something.</p> <p>P for Partition Tolerance. System still works even when network between servers breaks</p> <p>Example</p> <p>If some servers cannot talk to each other (network issue), the app should not fully shut down.</p> <p>This leads to trade-offs. For example, WhatsApp must always deliver messages (availability + partition tolerance). But sometimes messages appear out of order \u2192 consistency is temporarily sacrificed. No distributed large-scale system can fully satisfy all three in every situation.</p> <p>Exercise</p> <p>Provide an example of a scenario, where A or P is sacrificed.</p>"},{"location":"Lecture_Notes/08_database/#additional-material","title":"Additional Material","text":"<ul> <li>7 Database Paradigms</li> <li>Binary Search Algorithm in 100 Seconds</li> <li>SQL Explained in 100 Seconds</li> <li>ACID Properties in Databases With Examples</li> <li>CAP Theorem Simplified</li> <li>MongoDB in 100 Seconds</li> <li>Understanding B-Trees: The Data Structure Behind Modern Databases</li> <li>SQL vs NoSQL or MySQL vs MongoDB</li> <li>PostgreSQL in 100 Seconds</li> </ul> <ol> <li> <p>If you don't know what to choose, usually you cannot go wrong with PostgreSQL which also happens to be open-source.\u00a0\u21a9</p> </li> </ol>"},{"location":"Lecture_Notes/09_ai/","title":"09. Artificial Intelligence, Machine Learning, Deep Learning","text":"<p> Nilufar Ismayilova, Rumiyya Alili, Ismayil Shahaliyev Nov 29, 2025 Jan 31, 2026</p> <p>Artificial Intelligence (AI) refers to the field concerned with building computer systems that exhibit intelligent behavior. An AI system is one that can reason, plan, make decisions, solve problems, or act purposefully in an environment in pursuit of goals. The defining feature of AI is what the system does, not how it is implemented. AI systems do not need to learn from data: classical rule-based expert systems, symbolic logic engines, search and planning algorithms, and game-playing systems based on handcrafted rules are all examples of AI. Learning may be used in AI, but it is not required.</p>      How deep learning, machine learning and artificial intelligence are related. By Original file: Avimanyu786SVG version: Tukijaaliwa - File:AI-ML-DL.png, CC BY-SA 4.0, Link <p>Machine Learning is a methodological approach in which systems learn patterns or functions from data instead of being programmed with explicit rules. The central idea of machine learning is performance improvement through experience. Given examples, the system infers statistical regularities that allow it to make predictions or decisions on new data. Many machine learning systems perform narrow tasks such as regression, classification, clustering, or recommendation without reasoning, planning, or goal-directed behavior.</p> <p>Deep Learning is a specific class of machine learning methods based on multi-layer neural networks. Deep learning is especially effective for perceptual tasks such as image recognition, speech processing, and natural language understanding because it can automatically learn features directly from raw inputs. Deep learning is neither synonymous with intelligence nor with learning in general; it is a particular computational technique for large-scale function approximation within machine learning.</p> <p>Note</p> <p>Imagine an online clothing store that wants to show each customer personalized outfit suggestions. Instead of manually programming rules such as \"if user clicks blue shirts, recommend jeans\", the system can collect thousands of users' behaviors: items they viewed, purchased, returned, or saved to wishlist. Machine learning models then discover patterns, for example, customers who prefer minimalistic styles often buy neutral-colored items. Deep learning models can even analyze product images and understand patterns like \"striped,\" \"oversized,\" or \"casual.\" As a result, the AI system learns to automatically recommend items that match each person's style, even if no human explicitly defined the rules.</p> <p>Tip</p> <p>You can read more about deep learning on the website dedicated to the CSCI 4701: Deep Learning course taught at ADA University.</p>"},{"location":"Lecture_Notes/09_ai/#linear-regression-algorithm","title":"Linear Regression Algorithm","text":"Training a machine learning model.    <p>Linear Regression is one of the simplest machine learning algorithms. We assume that there is a linear relationship between the observed data and the prediction we wish to make. In that case, we can model the relationship between two variables by finding a straight line that best fits the data. The goal is to predict an output \\(\u0177\\) based on an input \\(x\\) using the function:  $$ \\hat{y} = wx + b $$</p> <p>Where \\(x\\) is the observed known input, \\(w\\) is the weight (slope) and \\(b\\) is the bias (intercept) of the function. The task of machine learning is to find the values of \\(w\\) and \\(b\\) that make the function's outputs match the ground-truth value (label) as closely as possible. Learnable values of a function are called model parameters. For comparison, GPT-3 model has 175 billion parameters, which were learned by training on a very large corpus of text collected from the World Wide Web and other publicly available sources.</p> <p>Note</p> <p>Suppose a company wants to predict a student's final exam score based on the number of hours they studied. It collects real data from previous students:</p> Study Hours (x) 5 10 13 20 Final Score (y) 45 55 82 94 <p>Let's say we provide random values for \\(w\\) and \\(b\\) to be \\(4.3\\) and \\(12\\). Then our model will make predictions according to the function: $$ \\hat{y} = 4.3x + 12 $$ Then a student who studies \\(19\\) hours is predicted to score: $$ 4.3(19) + 12 = 93.7 $$ The function our model uses is close to reality: we haven't observed any student studying for \\(19\\) hours before (we don't have it in our table), yet our model predicted a close performance similar to the observed student, who studied for \\(20\\) hours with an observed ground-truth label of \\(94\\) exam score.</p>"},{"location":"Lecture_Notes/09_ai/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>When training a machine learning model, we need a way to measure how well it is performing. This measurement is called the loss or error, and it tells us how far our predictions are from the actual values. The goal of training is to minimize this loss. However, calculating error is not as simple as it might seem. Consider this problem with this example dataset:</p> Study Hours (x) Actual Score [label] (y) Predicted Score (\u0177) Error (y \u2212 \u0177) 5 50 45 5 10 55 55 0 20 90 95 \u22125 <p>We calculated individual errors of our predictions by simply subtracting predicted scores from ground-truth values to see how much our model's output diverges from reality. But how can we measure average quality of our model's predictive power? A simple approach is to find arithmetic mean:  $$ \\frac{5 + 0 + (-5)}{3} = 0 $$ The mean error is zero, suggesting our model is perfect. But clearly, the model made errors on two predictions.The problem is that positive and negative errors cancel each other out.</p> <p>Note</p> <p>How would you avoid negative values cancelling each other out during averaging?</p> <p>To avoid this cancellation problem, we can take the absolute value of each error before averaging. Mean Absolute Error (MAE) will have the following solution: $$ \\mathrm{MAE} = \\frac{\\lvert 5 \\rvert + \\lvert 0 \\rvert + \\lvert -5 \\rvert}{3} = \\frac{10}{3} = 3.33 $$</p> <p>This is better because now we don't hide errors through cancellation. However, MAE treats all errors equally. A 10-point error and a 2-point error contribute proportionally to the total loss. To penalize large errors more heavily, we can square each error before averaging. Mean Squared Error (MSE) will have the following solution: $$ \\text{MSE} = \\frac{5^2 + 0^2 + (-5)^2}{3} = \\frac{25 + 0 + 25}{3} = 16.67 $$</p> <p>By squaring, large errors become disproportionately larger: an error of 2 becomes 4 (doubled), an error of 10 becomes 100 (10\u00d7 larger). This encourages the model to avoid making big mistakes. A problem with MSE, however, is that the units are squared. If we are predicting \"scores\", MSE gives us \"scores\u00b2\", which is not intuitive. To bring the error back to the original units, we take the square root: Root Mean Squared Error (RMSE) will have the following solution: $$ RMSE = \\sqrt MSE = \\sqrt 16.67 = 4.08 $$ Now our error is back in the original units, making it easier to interpret.</p>"},{"location":"Lecture_Notes/09_ai/#artificial-neural-networks","title":"Artificial Neural Networks","text":"<p>Artificial Neural Networks (ANNs) are inspired by how biological neurons in the human brain process information. Just like real neurons fire (get activated) and send electrical signals to the connected neurons, artificial neurons pass numerical values forward through the network.</p>      Biological Neuron. By User:Dhp1080 - \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance, Epidemiology and End Results (SEER) Program ., CC BY-SA 3.0, Link <p>To understand neural networks, it is helpful to start with a single neuron. An artificial neuron is a simple mathematical function. It takes one or more input values, multiplies each input by a corresponding weight, adds a bias, and produces an output. In its basic form, this is exactly a linear function with n inputs (e.g. study hours, amount of sleep, etc). $$ z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b $$ This linear combination is then passed through a nonlinear function called an activation function. For example, using the logistic (sigmoid) function, the final prediction of the neuron is: $$ \\hat{y} = \\frac{1}{1 + e^{-z}} $$ Note that we only did one additional step. Without the sigmoid function, our predictions would be based on a linear formula. We simply assume that our data is complicated or our data has a non-linear relationship between its inputs and output. It can also be noted that as the output of the linear function \\(z\\) approaches positive infinity, our prediction will approach 1, and in case of negative infinity, \\(0\\). Hence, the sigmoid function is limited to be in the range of [0,1] making our prediction probabilistic.</p>      Artificial Neuron. By Funcs - Own work, CC0, Link <p>A neural network is simply a collection (network) of such neurons. In its simplest form, multilayer perceptron is when an output of one neuron is an input of a neuron in the next layer. In the figure, the described neural network has three layers with nine neurons in total, where the output layer neurons make two predictions (e.g. probability of seeing a \"cat\", as well as a \"dog\" in the provided input image).</p>      Neural Network. By Glosser.ca - Own work, Derivative of File:Artificial neural network.svg, CC BY-SA 3.0, Link <p>Note</p> <p>Suppose we want to identify handwritten numbers (0\u20139) from images. The network has 10 output neurons, one for each digit. The neuron with the highest probability is the network's prediction.</p> Digit 0 1 2 3 4 5 6 7 8 9 Probability 0.01 0.02 0.01 0.00 0.01 0.10 0.82 0.01 0.01 0.01 <p>The model predicts the image is a 6 with 82% confidence.</p>"},{"location":"Lecture_Notes/09_ai/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication is one of the most important mathematical operations in machine/deep learning because it is the basic way neural networks process and transform information. A matrix is simply a rectangular grid of numbers, and multiplying two matrices means combining the rows of the first matrix with the columns of the second to produce new values. In a neural network, every layer uses matrices to store the weights that connect one layer of neurons to the next. When data flows through the network, the input values are represented as a vector, and this vector is multiplied by the weight matrix of the layer. The result of this multiplication is a new vector that represents the layer's output.</p> <p>All of these transformations, often millions of them in a single deep network, depend on matrix multiplication. Because neural networks require so many large matrix operations, hardware that can perform parallel computation is extremely valuable. This is why Graphics Processing Units (GPU) are widely used in machine/deep learning. Unlike CPUs, which process information sequentially, GPUs can perform thousands of small multiplications at the same time, making them ideal for the heavy matrix workloads found in modern AI systems.</p> <p>Note</p> <p>Suppose an image classifier needs to determine whether an image contains a cat or a dog. Each image is converted into a vector (list of numbers). If the input image has three features (simplified for explanation): $$ X = \\begin{bmatrix}  0.2 \\\\  0.9 \\\\ 0.4  \\end{bmatrix} $$ And the neural network layer has 3 neurons with weights: $$ W = \\begin{bmatrix}  0.5 &amp; 0.2 &amp; 0.1 \\\\ 0.9 &amp; 0.3 &amp; 0.5 \\\\  0.1 &amp; 0.8 &amp; 0.6  \\end{bmatrix} $$</p> <p>Then the output is simply: $$ W \\times X = \\begin{bmatrix}  0.5(0.2) + 0.2(0.9) + 0.1(0.4) \\\\ 0.9(0.2) + 0.3(0.9) + 0.5(0.4) \\\\  0.1(0.2) + 0.8(0.9) + 0.6(0.4)  \\end{bmatrix} $$</p> <p>The first neuron output: \\(0.1 + 0.18 + 0.04 = 0.32\\) The second neuron output: \\(0.18 + 0.27 + 0.20 = 0.65\\) The third neuron output: \\(0.02 + 0.72 + 0.24 = 0.98\\)</p> <p>So the final output vector is: $$ Y = \\begin{bmatrix}  0.32 \\\\  0.65 \\\\  0.98  \\end{bmatrix} $$</p> <p>These resulting numbers feed into activation functions and ultimately decide the probability it is a cat or a dog. This multiplication happens millions of times per second, which is why GPUs are essential.</p> <p>Tip</p> <p>For further information, see the content dedicated to the linear algebra.</p>"},{"location":"Lecture_Notes/09_ai/#large-language-models-llm","title":"Large Language Models (LLM)","text":"<p>Large Language Models (LLMs), such as GPT-based systems, are advanced deep learning models trained on extremely large collections of text. Through this training, they learn statistical patterns in how words, sentences, etc. relate to each other. Because of this, they can generate coherent text, answer questions, summarize long documents, translate between languages, and even write computer code. These abilities come from recognizing patterns across billions of examples, rather than being explicitly programmed with rules.</p> <p>LLMs are built using a special neural network architecture called the Transformer. The key component of this architecture is the attention mechanism, which enables the model to process all parts of the input in parallel. Instead of reading text sequentially from left to right, the model considers the entire sentences at once and computes how strongly each word is related to every other word. Because all words are processed simultaneously, transformers can efficiently model long-range dependencies without relying on step-by-step recurrence. This parallel structure makes training much faster on modern hardware and allows the model to capture relationships between distant words that would be difficult to retain in sequential models. As a result, large language models can represent context more accurately and generate coherent, context-aware text over long sequences.</p> <p>Note</p> <p>Consider a customer service chatbot for an internet provider. Customers commonly ask: \"My Wi-Fi is slow.\", \"Why is my internet disconnecting?\", \"How do I change my password?\"</p> <p>The company trains an LLM on: - thousands of past customer chat logs - network troubleshooting guides - modem/router manuals - internal support responses</p> <p>When a customer types: \"My internet randomly disconnects every hour\", the sentence is first converted into a sequence of numerical vectors called token embeddings. Each token (such as internet, disconnects, hour) is represented as a high-dimensional vector (list of numbers).</p> <p>These vectors are then processed in parallel by multiple attention layers. In each layer, the model computes how strongly every token relates to every other token using matrix operations. For example, the token disconnects may receive high attention weights from internet and hour, indicating a meaningful statistical relationship learned during training.</p> <p>Finally, based on these transformed representations, the model computes a probability distribution over the next possible words and generates a response token by token. The output appears coherent because the model has learned, from large-scale text data, which word sequences are statistically likely to follow similar inputs.</p> <p>Nowadays, AI is often treated as synonymous with neural network-based models (deep learning). This identification is misleading. Deep learning is a powerful tool within AI, not a definition of AI itself. Long before neural networks dominated the field, many AI systems were built using symbolic reasoning, search, and optimization methods that involve no learning from data at all.</p> <p>Examples include constraint satisfaction problems (CSPs), where the goal is to find assignments that satisfy a set of logical constraints; minimax search, used in game-playing agents to choose optimal moves by exploring possible future states; and reinforcement learning methods (such as Q-learning), which learn action values through interaction with an environment without relying on deep neural networks. These approaches remain central to AI and are still widely used in planning, scheduling, verification, and control.</p>"},{"location":"Lecture_Notes/09_ai/#csp-backtracking-algorithm","title":"CSP Backtracking Algorithm","text":"<p>Constraint Satisfaction Problems (CSPs) require assigning values to a set of variables in a way that satisfies all given rules. Each variable has a domain of possible values, and constraints specify which combinations of assignments are allowed. Backtracking is a systematic method for exploring all possible assignments. It works by choosing a variable, assigning it a value, and immediately checking whether this partial assignment violates any constraints. If the assignment is still valid, the algorithm moves to the next variable. If a violation appears at any point, the algorithm reverses its last step, removes the invalid value, and tries a different one.</p> <p>This process continues until either a complete valid assignment is found or all options have been exhausted. Backtracking is efficient because it avoids exploring entire branches of the search space as soon as a conflict is detected, instead of waiting until the end.</p> <p>Note</p> <p>Consider the classic Sudoku puzzle. Each empty cell must contain a number from 1-9 such that:</p> <ul> <li>No duplicate in the same row</li> <li>No duplicate in the same column</li> <li>No duplicate in the 3\u00d73 grid</li> </ul> <p>Backtracking works like this:</p> <ol> <li>Start at the first empty cell.</li> <li>Try placing \"1\" \u2192 check rules.</li> <li>If no rule is violated \u2192 move to next empty cell.</li> <li>If a violation occurs \u2192 remove the number (backtrack) and try \"2\", \"3\", ..., \"9\".</li> <li>If all numbers fail \u2192 backtrack to previous cell.</li> </ol> <p>This continues until the puzzle is complete. This same method is used in timetabling at universities, resource allocation, map coloring, and validating exam seating plans.</p>"},{"location":"Lecture_Notes/09_ai/#minimax-algorithm","title":"Minimax Algorithm","text":"<p>Minimax is an algorithm used in two-player, turn-based games where players take actions one after another, such as tic-tac-toe, chess, or checkers. The main idea is that one player is trying to choose moves that maximize their chance of winning, while the opponent is assumed to choose moves that minimize that chance. To decide the best move, the algorithm looks ahead at all possible future game states by simulating every move the player could make, every response the opponent could make, and so on. Each final outcome (win, loss, draw) is given a numerical score. Wins receive positive scores, losses receive negative scores, and draws get a neutral score.</p> <p>Once all these possible future outcomes are evaluated, the algorithm works backward through the game tree. At layers where it is our turn, Minimax selects the move that leads to the highest possible score. At layers where the opponent moves, the algorithm assumes the opponent will choose the option that gives us the lowest score. Following this alternating pattern of maximizing and minimizing ensures that the chosen move is the one that guarantees the best possible outcome even if the opponent plays perfectly.</p> <p>Note</p> <p>Assume it is the AI's turn and the board is following for tic-tac-toe:</p> <p> X O X O X O </p> <p>The AI (playing \"X\") considers every possible move. If it plays in the bottom-left corner, what are all the opponent's replies? If it plays in the middle-right cell, does it allow the opponent to win? If it plays incorrectly, does it allow a future fork? Minimax will:</p> <ul> <li>Assign +1 if the move leads to a guaranteed win</li> <li>Assign 0 if it leads to a draw</li> <li>Assign -1 if it leads to a forced loss</li> </ul> <p>The AI compares the scores of all possible futures. If the opponent can force a win, Minimax will avoid that move. This ensures the AI never loses, even against a perfect human player.</p> <p>Chess engines, Connect-4, and even some negotiation bots in economics use this principle. The highest rated and open-source chess engine Stockfish uses an advanced minimax search (now also combined with a neural network after its loss to AlphaZero).</p>"},{"location":"Lecture_Notes/09_ai/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement Learning (RL) is a branch of AI in which a system, called an agent, learns how to behave by interacting with an environment rather than by being shown the correct answers. The agent chooses an action, observes what happens as a result, and receives feedback in the form of a numerical reward or penalty. Good actions lead to positive rewards, while harmful or ineffective actions lead to negative ones. Over time, by repeatedly trying different actions and observing their consequences, the agent gradually learns which behaviors produce the highest long-term benefit. This learning process does not rely on explicit instructions. Instead, the agent discovers the best strategy on its own by balancing exploration of new actions with exploitation of actions that have worked well in the past.</p> <p>Key Concepts:</p> <ul> <li>Agent - the learner (a robot, a program, a game player)</li> <li>Environment - the world the agent interacts with</li> <li>Action - something the agent chooses to do</li> <li>Reward - numerical feedback (positive or negative)</li> <li>Policy - the strategy the agent learns</li> </ul> <p>Note</p> <p>Imagine training a robot to walk. At first, the robot moves randomly because it does not know what actions are helpful. It might take a step and fall, which produces a penalty, or it might shift its weight successfully and stay upright, which produces a reward. As these interactions continue, the robot begins to understand which sequences of movements keep it balanced and which cause it to fall. Eventually, after enough trial and error, the robot develops a stable walking pattern simply because that pattern consistently leads to higher rewards than others. In this way, RL enables systems to learn complex behaviors directly from experience rather than from predefined rules.</p>"},{"location":"Lecture_Notes/09_ai/#additional-material","title":"Additional Material","text":"<ul> <li>Linear Regression</li> <li>TensorFlow Playground</li> <li>TensorSpace Playground</li> <li>Tiktokenizer</li> <li>But what is a neural network? Deep learning chapter 1</li> <li>Machine Learning Explained in 100 Seconds</li> <li>Linear Regression in 3 Minutes</li> <li>AI vs ML vs DL vs Data Science - Difference Explained Simplilearn</li> <li>N-Queens - Backtracking - Leetcode 51 - Python</li> <li>Minimax: How Computers Play Games</li> <li>Matrix multiplication as composition Chapter 4, Essence of linear algebra</li> <li>Multiplying a matrix by a matrix Khan Academy</li> <li>Large Language Models explained briefly</li> <li>How I use LLMs</li> <li>Deep Dive into LLMs like ChatGPT</li> <li>Machine Learning Fundamentals: Bias and Variance</li> <li>How Cambridge Analytica Exploited the Facebook Data of Millions NYT</li> <li>AlphaGo - The Movie Full award-winning documentary</li> <li>AI Learns to Play Soccer (and breaks physics)</li> </ul>"},{"location":"Lecture_Notes/10_sdlc/","title":"10. Systems Development Life Cycle","text":"<p> Rahida Asadli, Rumiyya Alili, Ismayil Shahaliyev Dec 15, 2025 Jan 31, 2026</p> <p>Systems Development Life Cycle (SDLC) is a framework that guides the creation of an information system from initial idea to a working product. It describes the process of identifying system requirements, designing the system, building it, and deploying it to users.</p>"},{"location":"Lecture_Notes/10_sdlc/#participants-of-sdlc","title":"Participants of SDLC","text":"<p>Developing an information system is not just a technical task. It is a coordinated effort involving business decision-makers, technical specialists, and end-users, each with distinct responsibilities.</p> <p>Stakeholders are individuals or groups who are affected by the system or have an interest in its outcome. This includes managers, department heads, users, IT staff, and executives. Stakeholders provide input, set expectations, and evaluate whether the system delivers value.</p> <p>Project sponsor is a senior business representative who initiates the project by identifying a business need. The sponsor secures funding, supports the project at the executive level, and acts as the main business authority. Without a sponsor, a project usually lacks direction and protection.</p> <p>Steering committee (or approval committee) is a small group of senior managers and stakeholders. This group reviews major decisions, evaluates feasibility, approves budgets, and decides whether the project should start, continue, change direction, or be stopped. Their role is governance, not day-to-day work.</p> <p>Project manager is responsible for planning and controlling the project. This role creates schedules, manages costs, assigns tasks, tracks progress, and resolves risks. The project manager coordinates all participants and ensures that work across phases stays aligned with time, budget, and scope.</p> <p>Systems analyst focuses on how the information system supports the organization. This role studies current workflows, identifies problems, translates business needs into system requirements, and helps design the overall solution. Systems analysts bridge the gap between business and technology.</p> <p>Business analyst concentrates on business value and process improvement. This role examines how work is done today, identifies inefficiencies, defines new business rules, and ensures the system delivers measurable benefits. Business analysts speak the language of management and operations.</p> <p>Requirements analyst is responsible for gathering, documenting, and validating detailed system requirements. This role works closely with users and stakeholders to ensure requirements are clear, complete, and unambiguous. Good communication is critical here, because unclear requirements lead to system failure.</p> <p>Infrastructure analyst handles the technical foundation of the system. This includes servers, networks, databases, operating systems, and integration with existing systems. The infrastructure analyst ensures the system is reliable, scalable, and compliant with organizational standards.</p> <p>Change management analyst focuses on people, not technology. This role prepares users for the new system through training, documentation, communication, and support. The goal is to reduce resistance, ensure adoption, and help the organization adjust to new ways of working.</p> <p>Programmers and developers build the system based on the approved design. They write code, integrate components, and implement system functionality.</p> <p>Testers verify that the system works correctly. They check for errors, validate that requirements are met, and ensure the system behaves as expected under real conditions.</p> <p>Users (end-users) are the people who will use the system daily. They provide requirements, review designs, test early versions, and ultimately determine whether the system is successful. A system that users do not accept or trust will fail, regardless of technical quality.</p> <p>These labels describe roles, not fixed job titles. A role represents a set of responsibilities that must be performed for a system to be successfully developed, regardless of who performs them. In practice, organizations rarely use these exact titles. Instead, job titles emerge based on company size, structure, industry, and methodology. One person may carry out several roles under a single title, or a single role may be shared across multiple people. What matters is that the functions - decision-making, requirement definition, technical design, implementation, testing, and user adoption - are covered, even if the titles used in the organization are different or change over time.</p>"},{"location":"Lecture_Notes/10_sdlc/#phases-of-sdlc","title":"Phases of SDLC","text":"<p>The SDLC typically consists of four fundamental phases - planning, analysis, design, and implementation - each with its own steps, techniques, and deliverables.</p> <p>Note</p> <p>Think of it like building a house. First, you decide why you need the house and what problems it must solve - how many rooms it should have, who will live in it, and how much you can spend (planning). Next, you write down exact requirements: the number of floors, room sizes, plumbing, electricity, heating, and safety rules (analysis). Then you turn these requirements into precise drawings and technical plans that builders can follow, showing layouts, measurements, and materials (design). Finally, the house is built according to these plans, utilities are installed, and everything is checked to make sure it works as expected before people move in (implementation). Each step produces concrete results that guide the next step, and earlier decisions may be adjusted as new issues are discovered.</p>"},{"location":"Lecture_Notes/10_sdlc/#planning-phase","title":"Planning Phase","text":"<p>The main goal of the planning phase is to understand why an information system should be built and to decide how the project will be organized and managed. Before any system is designed or built, the organization must be sure that the project is necessary, realistic, and valuable.</p> <p>During project initiation, a business need or problem is identified. This need usually comes from a business department rather than the IT department. The idea for the new system is written in a document called a system request, which briefly explains the problem, the reason a system is needed, and how it is expected to help the organization. The person or department that proposes the system is known as the project sponsor.</p> <p>After the system request is prepared, a feasibility analysis is conducted. This analysis checks whether the project should move forward by examining three important areas.</p> <p>Technical feasibility asks whether the system can actually be built using the organization's current technology, skills, and resources: Can we build it?</p> <p>Note</p> <p>If a company wants to develop a mobile banking application, it must check whether it has developers with mobile programming skills, secure servers, and the ability to connect the app to existing banking systems. If these resources are missing, the project may not be technically feasible.</p> <p>Economic feasibility focuses on whether the system is worth its cost. It compares the expected benefits with the total costs of development, operation, and maintenance: Should we build it?</p> <p>Note</p> <p>If a supermarket plans to install self-checkout machines, it must consider whether the reduction in staff costs and faster checkout times will save more money than the cost of purchasing and maintaining the machines. If the benefits are greater than the costs, the project is economically feasible.</p> <p>Organizational feasibility examines how the system will fit within the organization and whether people will accept and use it. A system can fail even if it is technically and economically sound, simply because users resist it. If we build it, will users accept and use it?</p> <p>Note</p> <p>If a university introduces a new online attendance system, it must ensure that lecturers and students are willing to use it and that proper training is provided. Strong management support is also essential for organizational feasibility.</p> <p>Once the feasibility analysis is completed, the system request and analysis results are reviewed by an approval committee, often called a steering committee. This committee decides whether the project should be approved, changed, or rejected.</p> <p>If the project is approved, it moves into project management, which is the second step of the planning phase. At this stage, the focus shifts from deciding whether to build the system to deciding how the work will be carried out. The project manager breaks the project into specific tasks, estimates how long each task will take, identifies required resources, and assigns responsibilities to team members. A detailed schedule is created, showing task order, dependencies, milestones, and deadlines. The project manager also prepares a budget, identifies major risks, and defines how progress will be monitored and reported. This project plan becomes the roadmap for the entire system development effort and is used throughout the SDLC to track progress, control costs, and manage changes.</p>"},{"location":"Lecture_Notes/10_sdlc/#analysis-phase","title":"Analysis Phase","text":"<p>Analysis phase focuses on understanding the system in detail before it is designed or built. In this phase, the project team answers three key questions: who will use the system, what the system must do, and where and when it will be used. The goal is to clearly understand the current situation and define what the new system should achieve.</p> <p>During the analysis phase, the team first studies any existing system that is already in use. The team looks for problems, weaknesses, delays, errors, or user complaints. At the same time, they imagine how a better system could work in the future.</p> <p>Note</p> <p>If a company currently uses paper forms to track employee leave requests, the analysis phase would study how this process works now and then imagine a future system where requests are submitted and approved online.</p> <p>The next part of the analysis phase is requirements gathering. This means collecting information about what users and managers actually need from the system. The project team talks to users, managers, and other stakeholders to understand their expectations and daily work.</p> <p>Note</p> <p>When developing a university course registration system, analysts may talk to students, instructors, and administrative staff to understand how courses are selected, approved, and recorded. From this information, the team defines requirements, which clearly describe what the system must be able to do, such as allowing students to register for courses or enabling instructors to view class lists.</p> <p>Based on these requirements, the team develops a system concept. The system concept is a high-level description of how the new system will work and how it will support the business or organization. To explain this concept, analysts create simple models that show how users interact with the system and how information flows inside it. These models do not show technical details; instead, they help everyone understand the system in an easy and visual way.</p> <p>In the final step of the analysis phase, all findings are combined into a document called the system proposal. This document includes the analysis results, the system concept, the requirements, and the models. The system proposal is presented to the project sponsor and other decision makers, such as an approval committee. They use this document to decide whether the project should continue, be changed, or be stopped.</p> <p>The system proposal is the main output of the analysis phase. It is important to understand that this document does not only analyze the problem but also provides a high-level design idea for the new system. For this reason, some experts say this phase could be called \"analysis and initial design.\" However, it is commonly referred to as the analysis phase, and its result guides all later stages of system development.</p>"},{"location":"Lecture_Notes/10_sdlc/#design-phase","title":"Design Phase","text":"<p>Design phase focuses on deciding how the system will actually work. In this phase, the project team defines the technical details of the system, including the hardware and software to be used, the network setup, the user interface, the database structure, and the programs that must be created.</p> <p>One of the first tasks in the design phase is choosing a design strategy. This decision determines how the system will be developed. The organization may decide to build the system using its own programmers, outsource the development to an external company, or buy an existing software package and adapt it to its needs.</p> <p>After the design strategy is chosen, the team creates the system architecture design. This design describes the overall technical structure of the system, including the hardware, software, and network infrastructure. In many cases, the new system does not replace everything but instead connects to existing systems.</p> <p>At the same time, the team designs the user interface, which defines how users interact with the system. This includes screens, menus, buttons, forms, and reports. The goal is to make the system easy to use and understandable.</p> <p>The design phase also includes creating database and file specifications. These specifications clearly define what data will be stored, how it will be organized, and where it will be saved.</p> <p>Finally, the analyst team prepares the program design, which describes the programs that need to be written and what each program will do. This does not involve writing code yet; instead, it provides clear instructions for programmers.</p> <p>Note</p> <p>Consider the design of an online student registration system. During the design phase, the team decides whether to build the system internally or purchase and customize an existing solution. They define the system architecture, such as web servers for student access and database servers for storing course and enrollment data. The user interface design specifies how students search for courses, add or drop classes, and receive confirmation messages. Database specifications define tables for students, courses, enrollments, and prerequisites. Program designs describe modules for authentication, course validation, enrollment processing, and report generation.</p> <p>All design outputs are combined into a single document called the system specification. This document includes the architecture design, interface design, database specifications, and program design. It serves as the main guide for programmers during the next - implementation phase. At the end of the design phase, the feasibility analysis and project plan are reviewed again, and the project sponsor and approval committee decide whether the project should continue or be stopped.</p>"},{"location":"Lecture_Notes/10_sdlc/#implementation-phase","title":"Implementation Phase","text":"<p>In the implementation phase, the system is actually built and put into use. If the organization decided to buy an existing software package during the design phase, implementation includes installing and configuring that software instead of building it from scratch. This phase usually receives the most attention because it is often the longest and most expensive part of system development.</p> <p>The first step of implementation is system construction. During system construction, programmers write the code, databases are created, and system components are connected. At the same time, the system is carefully tested to make sure it works as expected. Testing is extremely important because fixing errors after the system is already in use can be very costly and disruptive. For example, if an online banking system has an error in transaction processing, it could lead to financial losses and loss of customer trust. For this reason, organizations often spend more time testing the system than writing the actual code.</p> <p>Once the system is built and tested, the next step is system installation. Installation is the process of switching from the old system to the new one. This may involve turning off the old system completely or running both systems at the same time for a short period. A key part of installation is user training. Users must learn how to use the new system correctly and confidently. For example, when a hospital installs a new patient record system, doctors, nurses, and administrative staff must be trained so that daily work is not interrupted and mistakes are avoided.</p> <p>The final step of the implementation phase is creating a maintenance (support) plan for the new system. This plan explains how the system will be supported after it goes live. It often includes a post-implementation review, where the project team checks whether the system meets its goals and whether users are satisfied. The support plan also defines who will fix problems, how updates will be handled, and how users can get help when they face difficulties.</p> <p>Together, these steps ensure that the system is not only built correctly but also successfully adopted and supported within the organization.</p> <p>Exercise</p> <p>Why follow a system development life cycle with phases described above?</p>"},{"location":"Lecture_Notes/10_sdlc/#approaches-to-sdlc","title":"Approaches to SDLC","text":"<p>The basic phases of the SDLC described above can be organised in different ways. Here we compare several common approaches, highlighting advantages and disadvantages.</p>"},{"location":"Lecture_Notes/10_sdlc/#waterfall-development","title":"Waterfall Development","text":"<p>Waterfall life cycle model is a traditional approach to systems development that follows a linear and sequential process. In this model, development moves step by step through clearly defined stages such as systems analysis \u2192 design \u2192 construction \u2192 testing \u2192 maintenance (support).</p> <p>Important</p> <p>Different textbooks often describe the SDLC phases and its approaches differently. In reality, there is no single official SDLC - it is a general framework, not a fixed standard. Authors adapt it to their teaching goals, historical context, and methodology preferences. As a result, phase names, the number of phases, and what is included in each phase can vary.</p> <p>Some textbooks describe four phases (planning, analysis, design, implementation). Others use five or six phases, separating implementation into construction and testing, or adding deployment and maintenance as separate phases. For example, maintenance may appear as part of implementation in one book and as its own phase in another.</p> <p>What stays consistent is the logic, not the labels: first understand the problem, then define requirements, then design a solution, then build it, test it, and put it into use. The ordering and purpose remain the same even when terminologies and boundaries differ.</p> <p>Each phase must be completed before the next one begins, and there is very little overlap between stages. A key assumption of the waterfall model is that system requirements remain stable after they are defined. This means that once the analysis phase is finished, changes are difficult and usually discouraged. Because of this, the waterfall model is most suitable for projects where requirements are well understood from the beginning and are unlikely to change. It is commonly used when risk must be tightly controlled and when strict documentation and formal approvals are required.</p> <p>The waterfall approach is often applied in large-scale and high-cost projects, where complexity is high and mistakes can be very expensive. The rigid structure helps ensure that every deliverable is carefully planned, documented, and reviewed. It is also useful for IT projects that do not involve software development, such as upgrading network equipment like routers and switches to support new technologies such as VoIP phones.</p> <p>One major advantage of the waterfall model is its clarity and structure. Because each phase has specific outputs, it is easy to manage, track progress, and control costs. However, a major disadvantage is its lack of flexibility. If requirements change later in the project, it can be costly and time-consuming to go back and make adjustments. This makes the waterfall model less suitable for projects where requirements are uncertain or evolving.</p> <p>Exercise</p> <p>What other advantages and disadvantages of the waterfall model can you define?</p>"},{"location":"Lecture_Notes/10_sdlc/#system-prototyping","title":"System Prototyping","text":"<p>System prototyping model performs analysis, design, and implementation concurrently to quickly produce a working prototype that users can evaluate. The first prototype is often a \"quick and dirty\" version with basic features. Users review it, provide feedback, and developers revise it. This cycle continues until the prototype has enough functionality to become the final system. This approach provides quick visible progress and helps users clarify requirements by interacting with a working system, which increases user confidence and involvement. However, because development starts quickly, there may be insufficient careful analysis early in the project, which can result in fundamental design weaknesses if requirements are not fully understood before implementation.</p> <p>Note</p> <p>A university develops a course registration prototype with simple search and enrollment features. Students test it, suggest improvements, and developers iteratively refine it until it becomes the production system.</p> <p>Throwaway prototyping uses prototypes as a learning tool that are discarded after serving their purpose, not evolved into the final system. After initial requirements analysis, the team builds quick design prototypes to explore alternatives, clarify uncertainties, and test technical feasibility. These may be simple mock-up screens or small proof-of-concept components. Once major questions are answered, prototypes are discarded and full development begins. This approach reduces risk and leads to more stable and reliable final systems because important problems are solved before real development begins. However, it can take longer to deliver since the prototypes themselves are not reused.</p> <p>Note</p> <p>Before developing a new hospital patient management system, the team builds a quick prototype that simulates patient registration, appointment scheduling, and record lookup using mock data and simple screens. The prototype is shown to doctors, nurses, and administrative staff to discover missing requirements, confusing workflows, and unrealistic assumptions. After this feedback is collected and the requirements are clarified, the prototype is discarded and the real system is designed and implemented using proper architecture, security, and performance standards.</p>"},{"location":"Lecture_Notes/10_sdlc/#minimum-viable-product-mvp","title":"Minimum Viable Product (MVP)","text":"<p>Minimum Viable Product (MVP) is a special form of prototyping used in startups and innovation projects. It is the simplest version of a product that delivers value to early users and allows the team to test assumptions. The MVP includes just enough features to solve the core problem; other features are deferred. By releasing an MVP, teams learn what customers really need, reduce waste and iterate quickly.</p> <p>Note</p> <p>Imagine designing an online bookstore. A system prototype might include simple screens to search for books and add them to a cart. Users try it and suggest improvements. A design prototype might be a set of static HTML pages showing how the checkout process could look; the pages do not process actual orders but help clarify layout and wording.</p> <p>Although prototyping and MVP development both involve iterative refinement, they differ in purpose. The system prototyping model is used primarily to refine and stabilize system requirements within an organization, with the prototype gradually evolving into the final system. An MVP, in contrast, is used to validate a product's value and business viability with real users in a market setting. While prototyping focuses on building the right system, an MVP focuses on determining whether the system is worth building at all.</p> <p>Exercise</p> <p>Describe advantages and disadvantages of prototyping and MVP approaches over the waterfall model.</p>"},{"location":"Lecture_Notes/10_sdlc/#agile-development","title":"Agile Development","text":"<p>Agile development approach was created in response to the limitations of traditional models like waterfall, especially in environments where requirements are unclear or change frequently. Agile emphasizes flexibility, collaboration, and continuous improvement rather than rigid planning. In agile projects, requirements and solutions evolve through close cooperation between developers and users.</p> <p>Agile development focuses on delivering working software in small parts, rather than delivering everything at the end of the project. User feedback is collected regularly, and changes are welcomed, even late in development. This makes agile particularly suitable for software development, mobile applications, and systems where customer needs are expected to change.</p> <p>A key idea behind agile is described in the Agile Manifesto, which values people and communication over strict processes, working software over extensive documentation, customer collaboration over rigid contracts, and responding to change over following a fixed plan. These values guide how agile is applied, but each organization is responsible for interpreting and using them in practice.</p> <p>One popular agile method is Scrum, which organizes work into short time periods called sprints, usually lasting two to four weeks. During each sprint, a small set of features is developed, tested, and prepared for delivery. At the end of each sprint, the team reviews progress and reflects on how to improve in the next cycle. This repeating process continues until the project goals are met, the budget is exhausted, or a deadline is reached.</p> <p>The main advantages of agile development are its flexibility, strong user involvement, and ability to deliver value quickly. Agile helps ensure that the most important features are completed first and that the system stays aligned with customer needs. However, agile development requires active user participation, experienced team members, and a cultural shift within the organization. Without proper coordination, agile projects can suffer from unclear scope and difficulty in long-term planning.</p> <p>Exercise</p> <p>What other advantages and disadvantages of agile development can you define?</p>"},{"location":"Lecture_Notes/10_sdlc/#unified-modeling-language-uml","title":"Unified Modeling Language (UML)","text":"<p>Unified Modeling Language (UML) is a standardized visual language used to analyze, design, and communicate how an information system should work. Its main purpose is to make system requirements and design decisions clear and unambiguous for both technical and non-technical stakeholders. UML is used mainly during the analysis and design phases of the SDLC, where it helps teams reason about system scope, structure, and behavior before implementation begins.</p>      A UML class diagram of a library system.    <p>Class diagram is used mainly during system design to describe the static structure of the system. It shows classes, their attributes, and the relationships between them. For example, in a library system, classes such as Book, Member, and Loan are defined. The Book class may be specialized into PrintedBook and EBook, both inheriting common attributes such as title and identifier. The diagram also shows how a Loan connects a Member to a Book, providing a clear basis for database design and program structure.</p> <p>Sequence diagram is used to describe how objects interact over time to complete a specific use case. It shows the order of messages exchanged between objects during execution. For example, a sequence diagram for the \"Borrow Book\" use case can show how the user interface sends a request, the system checks book availability, creates a loan record, updates the book's status, and confirms the transaction. This diagram helps developers understand control flow and responsibilities during runtime.</p> <p>Exercise</p> <p>Create a use case diagram, class diagram, and sequence diagram for an online course registration system that allows students to enroll in courses, drop courses, view schedules, and allows administrators to manage courses and student records.</p>"},{"location":"Lecture_Notes/10_sdlc/#software-testing","title":"Software Testing","text":"<p>Software testing is the process of evaluating a system to determine whether it functions correctly and meets specified requirements. The main goals of testing are to detect defects, verify that requirements are satisfied, and reduce the risk of failure after deployment. Testing is performed throughout the SDLC, with different types of testing focusing on different levels of the system.</p> <p>Unit testing focuses on testing individual units of code, such as functions, methods, or classes, in isolation. It is usually performed by developers during the implementation phase. The purpose of unit testing is to ensure that each small component behaves correctly before it is integrated with others. For example, a unit test may verify that a function calculating late fees returns the correct amount for various inputs.</p> <p>Integration testing examines how different modules or components work together. Even if individual units pass unit tests, errors can occur when components interact. Integration testing focuses on data flow, interfaces, and communication between modules and is typically performed after unit testing. For example, integration testing can verify that borrowing a book correctly updates both the loan records and inventory data.</p> <p>System testing evaluates the complete and integrated system as a whole. It checks whether the system meets its functional and non-functional requirements, such as performance, security, and reliability. System testing is often performed by a testing team or quality assurance (QA) group before the system is delivered to users.</p> <p>User Acceptance Testing (UAT) is performed by end users or customer representatives to confirm that the system supports real business tasks and is ready for operational use. UAT focuses on business correctness rather than technical details. For example, librarians may perform UAT to ensure that all daily library operations can be completed correctly using the new system.</p> <p>Regression testing ensures that recent changes or bug fixes have not broken existing functionality. It involves re-running previously executed tests and is especially important in systems that evolve over time. Regression testing helps maintain system stability as new features are added.</p>"},{"location":"Lecture_Notes/10_sdlc/#extreme-programming-xp","title":"Extreme Programming (XP)","text":"<p>Extreme programming (XP) is an agile development approach that emphasizes close collaboration, continuous feedback, and high-quality code. Its goal is to deliver useful software quickly while remaining flexible to changing requirements. XP encourages frequent interaction between developers and users so that misunderstandings are identified early.</p> <p>Short iterations deliver working functionality regularly. Pair programming improves code quality and knowledge sharing by having two developers work together. Unit testing is central, ensuring that every component behaves correctly and enabling safe changes. Continuous integration ensures that new code is frequently combined and tested to avoid late-stage failures.</p>"},{"location":"Lecture_Notes/10_sdlc/#joint-application-development-jad","title":"Joint Application Development (JAD)","text":"<p>Joint Application Development (JAD) is a structured technique used to gather and define system requirements through intensive, facilitated workshops. Instead of collecting requirements through long sequences of interviews, JAD brings users, managers, analysts, and developers together to reach shared understanding and quick decisions.</p> <p>JAD is typically used during the analysis phase, especially for systems with many stakeholders or complex business rules. A trained facilitator guides the sessions, keeps discussions focused, and ensures that all viewpoints are heard. The outcome is a clearer, more complete set of requirements and stronger user ownership of the system.</p> <p>Note</p> <p>For example, when developing a company-wide reporting system, representatives from finance, operations, and management participate in JAD sessions together with systems analysts. During these sessions, they agree on required reports, data definitions, and business rules. This shared understanding early in the project reduces later changes and increases acceptance of the system.</p>"},{"location":"Lecture_Notes/10_sdlc/#additional-material","title":"Additional Material","text":"<ul> <li>Software Development Life Cycle: Explained</li> <li>Estimate Software Development Costs</li> <li>Software Planning and Technical Documentation</li> <li>What is Agile?</li> <li>UML use case diagrams</li> <li>UML class diagrams</li> <li>Software Testing Explained in 100 Seconds</li> </ul>"},{"location":"Lecture_Notes/11_ethics/","title":"11. Ethics, Security, and Logical Reasoning","text":"<p> Nilufar Ismayilova, Rumiyya Alili, Ismayil Shahaliyev Nov 29, 2025 Jan 31, 2026</p>"},{"location":"Lecture_Notes/11_ethics/#space-shuttle-challenger-disaster","title":"Space Shuttle Challenger Disaster","text":"<p>The most dangerous phrase in the language is: \"We've always done it this way\" ~ Grace Hopper</p> <p>One of the most important historical examples used to discuss ethics in engineering and information systems is the Space Shuttle Challenger disaster (1986). The Challenger shuttle broke apart shortly after launch, resulting in the deaths of all seven crew members. </p>      STS-51-L crew: (back) Onizuka, McAuliffe, Jarvis, Resnik; (front) Smith, Scobee, McNair. By NASA - NASA Human Space Flight Gallery (image link), Public Domain, Link <p>Investigations later showed that the disaster was not caused by a single technical failure, but by a combination of engineering, organizational, and ethical failures. A critical technical issue involved O-ring seals in the solid rocket boosters.</p> <p>These O-rings were responsible for preventing hot combustion gases from escaping during launch. This design created a single point of failure: if the O-ring did not seal properly, there was no additional system to contain the pressure. One component became responsible for the safety of the entire mission.</p> <p>Before the Challenger launch, engineers from Morton Thiokol analyzed data from previous shuttle flights and noticed that the O-rings performed worse at lower temperatures. During a pre-launch meeting, engineers explicitly recommended delaying the launch because the temperature on launch day was far below any previous flight conditions. However, NASA managers were under pressure to maintain the launch schedule and asked engineers to provide proof that the launch would fail, instead of proof that it was unsafe. Communication problems, schedule pressure, and fear of delaying the mission played a major role in ignoring these warnings.</p>      Cross-sectional diagram of the original SRB field joint. The top end of the lower rocket segment has a deep U-shaped cavity, or clevis, along its circumference. The bottom end of the top segment extends to form a tang that fits snugly into the clevis of the bottom segment. Two parallel grooves near the top of the clevis inner branch hold ~20 foot (6 meter) diameter O-rings that seal the gap between the tang and the clevis, keeping hot gases out of the gap. By Rogers Commission into the loss of the Space Shuttle Challenger - http://history.nasa.gov/rogersrep/v1ch4.htm, Public Domain, Link <p>This phenomenon is called normalization of deviance. NASA had seen O-ring erosion in earlier flights but accepted it as \"normal\" because flights had not failed yet. When warning signs appear repeatedly without immediate catastrophic consequences, organizations can begin to accept abnormal and risky conditions as normal. This creates a culture where critical safety concerns are downplayed, and each successful outcome despite the warnings reinforces the dangerous belief that \"it will be fine this time too.\"</p> <p>Exercise</p> <p>Identify the key ethical mistakes that occurred in the Challenger shuttle case and explain what engineers and decision-makers should have done differently to ensure public safety and proper handling of technical concerns.</p>"},{"location":"Lecture_Notes/11_ethics/#trolley-problem","title":"Trolley Problem","text":"<p>Ethical questions in technology (and life) often involve difficult trade-offs, where every available option leads to some form of harm or disadvantage. One well-known thought experiment used to explore moral reasoning in such situations is the trolley problem. In its basic form, a runaway trolley is heading toward several people standing on a track. You have the option to pull a lever that redirects the trolley onto another track, where it will harm only one person instead. The dilemma forces individuals to choose between actively causing harm to one person or passively allowing greater harm to occur.</p>      The trolley problem presents a dilemma: is it preferable to pull the lever to divert the runaway trolley onto the side track with just one person? By Original:  McGeddon\u2002Vector: Zapyon - This SVG diagram includes elements from this icon:, CC BY-SA 4.0, Link <p>The trolley problem has no simple or universally correct answer. Instead, it highlights how ethical decisions often involve conflicting values, such as minimizing harm, respecting individual rights, and determining responsibility for outcomes. Some people focus on the consequences of actions and choose the option that saves the most lives, while others believe it is morally wrong to actively cause harm, even if it leads to a better overall outcome. These different responses demonstrate how ethical reasoning can vary depending on personal values, cultural background, and situational context.</p> <p>Consequentialism judges decisions by their outcomes. A consequentialist, especially a utilitarian, might argue that diverting the trolley is morally right because it reduces the total loss of life. The focus is on maximizing overall well-being, even if doing so involves harming an individual.</p> <p>Deontological ethics evaluates actions based on rules, duties, and respect for individual rights. From this view, deliberately causing a person's death - by pulling the lever or pushing someone - is morally wrong regardless of the positive result. The right action is determined not by consequences but by adherence to moral principles.</p> <p>The scenario also engages the doctrine of double effect, which distinguishes between harm that is foreseen but unintended and harm that is used as a means to an end. Saving five lives while unintentionally causing death may be seen differently from intentionally killing one person to save five. Another important idea is agent responsibility: whether moral judgment depends on what we do directly versus what we merely allow to happen. Some argue that actively intervening and causing harm makes us more morally responsible than if harm occurs without our direct action.</p> <p>In the context of information systems, artificial intelligence, and automation, similar dilemmas arise in real-world system design. For example, engineers developing autonomous vehicles must consider how a system should behave in unavoidable accident scenarios. Should the system prioritize the safety of passengers inside the vehicle, pedestrians outside, or follow traffic laws strictly regardless of the outcome? Likewise, algorithmic systems may face ethical trade-offs between maximizing efficiency and fairness, such as optimizing delivery routes at the cost of increased pollution in certain neighborhoods or using automated decision systems that unintentionally disadvantage specific groups.</p> <p>Note</p> <p>Consider a self-driving car approaching an unavoidable accident. If the car continues forward, it will collide with several pedestrians crossing the street illegally. If it swerves, it will crash into a barrier, putting the passenger at serious risk. Engineers must decide how the system should behave in this situation. Should the algorithm prioritize minimizing total harm, protecting the passenger, or following traffic laws strictly? Different answers lead to different ethical outcomes. What would you choose? Why?</p>"},{"location":"Lecture_Notes/11_ethics/#deduction-and-induction","title":"Deduction and Induction","text":"<p>Logical reasoning plays a central role in engineering, information systems, and ethical decision-making. From designing algorithms to evaluating system security and making responsible choices, professionals rely on logic to analyze problems and justify conclusions. Many of the foundational ideas behind logical reasoning were first systematized by Aristotle, who is often regarded as the founder of formal logic. His work established structured methods for evaluating arguments and distinguishing valid reasoning from flawed or misleading conclusions.</p> <p>Aristotle introduced deductive logic, particularly through the concept of the syllogism, which is a structured form of argument consisting of premises that lead to a necessary conclusion. In deductive reasoning, the argument moves from general principles to specific cases. If the premises are true and the logical structure is valid, the conclusion must also be true. This type of reasoning is especially important in system design and formal verification, where certainty and correctness are required.</p> <p>Note</p> <p>Below is perhaps the most famous syllogism:</p> <p>All humans are mortal. Socrates is a human. Therefore, Socrates is mortal.</p> <p>The conclusion (third line) follows logically from the two premises. The structure correctly moves from a general rule to a specific case.</p> <p>Exercise</p> <p>Is the argument below logically sound? If not, identify what is wrong with its reasoning.</p> <p>All secure systems require authentication. This system requires authentication. Therefore, this system is secure.</p> <p>In contrast, inductive reasoning moves from specific observations to broader generalizations. Rather than guaranteeing truth, induction produces conclusions that are probable or likely, based on patterns in observed data. Aristotle recognized induction as a key method for gaining knowledge about the world, especially when universal rules are not yet established.</p> <p>Note</p> <p>This system failed after several cyber attacks. Similar systems also failed after attacks. Therefore, such systems are likely vulnerable.</p> <p>Here, the conclusion is reasonable but not absolutely certain. A future system might behave differently, or additional factors may influence outcomes.</p> <p>Exercise</p> <p>Can you bring an example of inductive reasoning in information systems, where conclusions are drawn from historical data rather than strict rules?</p> <p>Modern science relies primarily on induction. We observe patterns in data and infer general laws or predictions. Engineers test systems repeatedly and assume that components that behaved reliably before will continue to do so. Security analysts look at previous cyberattacks to estimate future threats. Machine learning itself is grounded in induction: models infer patterns from past samples in the hope that those patterns generalize to new cases. This approach is powerful - but it never guarantees certainty.</p> <p>Important</p> <p>Mathematical induction is actually a deductive proof method, not induction from repeated observations. Meanwhile, the so-called \"deductions\" of Sherlock Holmes are typically inductive generalizations from clues and experience rather than strict logical necessity.</p> <p>This reliance on induction raises a major philosophical issue identified by David Hume. Inductive conclusions assume that the future will resemble the past, yet there is no logical proof for this assumption. We expect the sun to rise tomorrow because it has always risen before, but that reasoning only works if we already assume the future behaves like the past. Hume called this circular: we justify induction by appealing to past success, which is itself an inductive reasoning.</p> <p>Karl Popper later emphasized that no amount of positive evidence can conclusively prove a scientific theory, but a single counterexample can disprove it. This idea was popularized through the \"black swan\" illustration: after centuries of observing only white swans, Europeans concluded that all swans must be white - until the discovery of black swans in Australia instantly falsified that belief. Popper argued that progress in science comes not from proving theories true, but from exposing them to conditions under which they could be shown false. A claim that cannot, even in principle, be falsified does not count as scientific.</p> <p>Hume's challenge motivated the search for certainty through deduction. If observation cannot guarantee truth, perhaps truth could be established through pure reasoning. Mathematics follows this strategy: it begins with axioms - statements accepted as foundational - and applies strict logical rules to derive conclusions. If the axioms are true, then the conclusions must also be true. Deductive reasoning does not rely on the future behaving like the past; its certainty comes from structure, not observation.</p> <p>Important</p> <p>Even mathematical certainty is limited by the axioms that define a system. Change those axioms, and the truths derived from them change as well. For example, in Euclidean geometry, based on Euclid's fifth postulate, parallel lines are defined as lines that never intersect. But in spherical geometry, the closest analogue to straight lines are great circles - and every pair of great circles intersects - so parallel lines do not exist at all. Both geometries are internally valid because each follows logically from different foundational assumptions. Deductive certainty is therefore conditional and applies only within the chosen axiomatic framework, not as a universal reflection of reality. Deductive conclusions are guaranteed only if the starting assumptions are.</p>"},{"location":"Lecture_Notes/11_ethics/#ambiguity-of-language","title":"Ambiguity of Language","text":"<p>Natural language is inherently imprecise. Many disagreements in engineering, science, and ethics arise not from factual disputes but from differences in how people interpret the same words. Terms such as \"secure,\" \"harm,\" or \"responsibility\" may appear obvious, yet their meaning can shift dramatically depending on context.</p> <p>To avoid such misunderstandings, we may attempt to translate vague statements into precise logical form. Mathematics is especially attractive for this goal because, at least within a defined axiomatic system, it provides deductive certainty: if the premises are true and the reasoning is valid, the conclusion must also be true.</p> <p>Philosophers have long recognized this problem and sought ways to make reasoning more reliable. Aristotle took the first major step by abstracting arguments away from their wording and focusing on their form. He expressed reasoning patterns using generalized terms, similar to variables in modern logic:</p> <p>All A are B.  C is A.  Therefore, C is B.</p> <p>This structure is valid no matter what A, B, and C refer to. In this role, these letters function much like variables in programming or algebra: they stand in for any object or concept, allowing the logical structure to be tested independently of the specific content.</p> <p>Aristotle's innovation laid the foundation for formal logic by showing that correct reasoning can be analyzed independently of ordinary language. As scientific knowledge expanded, ambiguity became an increasingly serious obstacle. Philosophers aimed to construct systems where truth could be determined without relying on interpretation. Gottlob Frege attempted to rebuild reasoning using a symbolic language (see example above) that separated meaning from grammar.</p> <p>Bertrand Russell continued this project in the effort to eliminate contradictions from mathematics by grounding it entirely in logic. Famously, in Principia Mathematica, authored by Russell and his teacher Alfred North Whitehead, \\(1 + 1 = 2\\) is finally proved after a few hundred pages of formal logical development. Their work influenced the Vienna Circle, who argued that every meaningful statement must be expressible in exact logical form. They hoped that by reducing knowledge to logical structures - essentially advanced kinds of AND/OR relationships used in digital circuit - reasoning itself could become precise, verifiable, and immune to error. If all argumentation could be translated into symbolic logic, rational thought could in principle be automated and made reliable.</p> <p>However, this project revealed important limitations. Even if the structure of an argument is perfectly logical, its symbols must still be defined by humans. The meaning of key terms remains uncertain, especially in ethical or social contexts. Context can alter interpretation dramatically, and real-world decisions often involve conflicting goals and values that cannot be fully captured by rigid rules.</p>"},{"location":"Lecture_Notes/11_ethics/#asimovs-laws-of-robotics-laws","title":"Asimov's Laws of Robotics {#laws}","text":"<p>The prolific science-fiction writer and professor of biochemistry Isaac Asimov proposed the Three Laws of Robotics in 1942 as a framework for robot behavior, after editor John W. Campbell recognized a consistent pattern in Asimov's stories and encouraged him to formalize those principles as explicit laws:</p> <ol> <li>A robot may not injure a human being or, through inaction, allow a human being to come to harm.</li> <li>A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.</li> <li>A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.</li> </ol> <p>In structure, these laws resemble boolean algebra or programming statements: IF/THEN, AND/OR rules that one would encounter in basic propositional logic or in early digital logic design: IF a robot must obey humans AND must NOT harm humans, THEN commands that would cause harm are NOT allowed. The laws function like conditional chains in a circuit.</p> <p>While the laws seem logical and protective, Asimov's own stories showed how these clean, binary rules break down, and produce unintended and sometimes dangerous consequences. Consider the First Law's prohibition against allowing humans \"to come to harm\". In Asimov's fiction, robots interpreting this literally concluded they must protect humans from all possible danger - including themselves. This led robots to imprison humans, restricting freedom and choices to eliminate any risk of harm. The rule was followed perfectly, but the outcome violated human well-being.</p> <p>This is an early representation of the AI alignment problem: aligning machine behavior with human values is not solved by simply giving a machine a bunch of hand-crafted rational rules. Modern AI alignment research faces the same dilemma. We can define rules, objectives, or reward functions, yet systems may still follow them in ways that violate human expectations. Language is imprecise, values conflict, and context continually changes. Algorithms alone cannot ensure safe or morally responsible behavior.</p> <p>Note</p> <p>An AI programmed to \"maximize user engagement\" on social media might discover that divisive, inflammatory content keeps people engaged longer. The algorithm optimized for its stated goal perfectly, but the outcome was harmful: increased polarization and spread of misinformation.</p> <p>Exercise</p> <p>Design an improved version of the engagement algorithm noted in the example above by specifying a clearer objective function. Include constraints that protect user well-being, reduce misinformation, and prevent harmful content from being promoted, while still allowing the system to recommend interesting material.</p> <p>The deeper issue is still the inherent ambiguity of natural language. Words like \"harm,\" \"obey,\" and \"protect\" appear to carry precise meaning, but when applied in complex environments, they require interpretation. A robot asked to prevent harm may choose solutions humans consider outrageous. A system told to obey orders might follow dangerous commands literally.</p> <p>We mentioned how logicians pursued a vision of language modeled on mathematics - free from vagueness, contradiction, or subjective interpretation. The aim was certainty through formalization: if all knowledge could be translated into exact logical structures, then reasoning could become universally valid and immune to misunderstanding. But real ethics and real language resist such reduction. Mathematical logic works with well-defined symbols and truth values; ethical decisions involve contested principles, emotional context, and unpredictable consequences. You cannot represent every nuance in an AND/OR gate.</p> <p>Asimov's laws demonstrate a crucial insight in applied ethics and technology design: even rules that appear perfectly rational can fail when language is imprecise, human values conflict, or real-world context complicates their interpretation. Hand-crafted rules cannot anticipate every scenario, resolve every conflict, or encode the full complexity of human moral life.</p>"},{"location":"Lecture_Notes/11_ethics/#acm-code-of-ethics","title":"ACM Code of Ethics","text":"<p>Decisions made by computing professionals can protect users or put them in danger. A system that performs well but leaks personal information, or an AI that boosts engagement by amplifying harmful content, may succeed technically but still fail the people who rely on it. Good engineering requires more than correct algorithms.</p> <p>Professional organizations such as the Association for Computing Machinery (ACM) provide codes of ethics to guide these decisions. These codes emphasize honesty, accountability, protection of user privacy, and the duty to prevent harm, even when it conflicts with business goals or tight deadlines. Because we cannot encode every ethical rule into formal logic, the ACM Code relies on the assumption that computing professionals will apply fair and responsible human judgment when facing unclear or conflicting situations.</p> <p>We haven't yet found a way to reduce ethics to formulas, logical gates, or automated decision rules. Ethical judgment requires human interpretation, awareness of context, and attention to values that may compete or change over time. Professionals must therefore combine technical skill with ethical judgment. Logical reasoning helps detect inconsistencies and foresee risks, but ethical awareness ensures that performance improvements do not come at the expense of human dignity, privacy, or safety. Responsible design means anticipating consequences - not just solving technical problems - and accepting accountability for the systems we create.</p>"},{"location":"Lecture_Notes/11_ethics/#viruses-and-cyber-attacks","title":"Viruses and Cyber Attacks","text":"<p>Technological advancement is not inherently good or bad - it is neutral. Every innovation expands what humans can do, and that includes what humans can do to harm. The same network that enables global collaboration can also enable global exploitation. The same cryptography that protects dissidents can also shield criminals. Attackers exploit weaknesses not because technology is flawed, but because human purposes differ: some aim to create, others to manipulate or disrupt. When power increases faster than accountability, the potential for harm grows.</p> <p>Having positive ethical intentions alone is not enough. A system that works flawlessly under honest use may crumble under malicious intent. Attackers push technology to its edges, probing for assumptions we failed to consider - assumptions about behavior, trust, incentives, or access. Their actions reveal the truth that every technological system embodies both possibility and vulnerability.</p> <p>Computing professionals must therefore understand how systems are attacked, not only how they are designed. Protecting users requires anticipating misuse, resisting manipulation, and acknowledging that technology's benefits are inseparable from its risks.</p> <p>Social engineering attacks focus on people rather than code. Phishing messages may impersonate banks or universities to trick users into giving away credentials. Closely related techniques like spear-phishing can compromise entire organizations. These attacks succeed because trust is easier to manipulate than technology.</p> <p>Malware attacks exploit weaknesses in software and system configurations. Trojan horse can appear as a legitimate software applications and perform malicious actions. Ransomware encrypts data and demands payment. Worms spread automatically across networks, while spyware collects private information silently. Despite their differences, all malware seeks control, disruption, or theft.</p> <p>Network attacks target communication itself. Man-in-the-Middle (MITM) attack intercepts or alters data. Denial-of-Service (DoS) attacks overwhelm servers so legitimate users cannot access services. These attacks show that even secure software can fail if communication channels are not protected.</p> <p>Credential stuffing uses leaked usernames and passwords from one service to break into accounts on another. Brute-force attacks systematically guess passwords until one succeeds. These threats work not through sophisticated hacking, but through human error and poor security design.</p> <p>Wipers permanently erase data for sabotage. Clickjacking disguises harmful actions behind innocent clicks. Typosquatting exploits small URL mistakes to redirect victims to fake sites. These attacks undermine trust and enable fraud or political manipulation.</p> <p>Note</p> <p>A student downloads what appears to be a free PDF textbook from an unofficial website. The file secretly contains a Trojan that installs itself on the laptop. Once installed, the Trojan records keystrokes and sends login credentials to an attacker. Later, the same laptop becomes part of a botnet and is used to send traffic during a DoS attack against a university server.</p>"},{"location":"Lecture_Notes/11_ethics/#digital-hygiene","title":"Digital Hygiene","text":"<p>Digital hygiene refers to everyday habits and practices that help users protect their devices, personal data, and online identity. Similar to personal hygiene, it focuses on regular and simple actions that prevent long-term harm rather than reacting only after problems occur. Good digital hygiene reduces the likelihood of cyber attacks, data loss, and unauthorized access, especially as users increasingly rely on digital systems for communication, study, and financial activities.</p> <p>Some basic recommendations include using strong and unique passwords for different accounts. A single reused password becomes a master key: if one service is breached, attackers can access email, banking, and other vital accounts. Password managers help solve this problem by generating and storing unique passwords securely. However, they create a single point of failure: if the master password is weak, forgotten, or stolen, every account stored in the password manager may be compromised at once. Enabling multi-factor authentication (MFA) adds an extra layer of security by requiring a second verification step, such as a code sent to a phone.</p> <p>Users should avoid relying on any single component of their digital life. If all important files exist only on one laptop or one server, a hardware failure, ransomware attack, or accidental deletion could permanently destroy access. Regular backups - stored offline or in a trusted cloud environment - ensure that critical data can be recovered even if the primary device fails. The goal is to avoid single points of failure anywhere that matters.</p> <p>Keeping software, applications, and operating systems updated is essential, as updates often fix known security vulnerabilities. Users should avoid clicking suspicious links or downloading files from unknown sources, since these remain common delivery methods for malware. Caution is also needed when sharing personal information online, especially on social media, where attackers collect details to craft convincing phishing attacks.</p> <p>Using secure and trusted networks further reduces exposure to eavesdropping and interception, particularly when accessing sensitive accounts such as online banking or university services. Together, these practices form a proactive defense: small habits that prevent major harm.</p>"},{"location":"Lecture_Notes/11_ethics/#additional-material","title":"Additional Material","text":"<ul> <li>Challenger Disaster</li> <li>Consequentialism vs. Deontology</li> <li>Absurd Trolley Problems</li> <li>AI Decides on Absurd Trolley Problems</li> <li>David Hume and the Problem of Induction</li> <li>The Ascent of Money Episode 3: Blowing Bubbles</li> <li>Aristotle: Logic Internet Encyclopedia of Philosophy</li> <li>Isaac Asimov - Laws of Robotics - Extra Sci Fi</li> <li>ACM Code of Ethics</li> <li>Theranos - Silicon Valley's Greatest Disaster</li> <li>Every Type of Computer Virus Explained in 8 Minutes</li> <li>Every Type of Cyber Attack Explained in 8 Minutes</li> <li>Digital hygiene</li> </ul>"},{"location":"course/2025_fall/","title":"Fall 2025","text":""},{"location":"course/2025_fall/#schedule","title":"Schedule","text":"Day Time 10425 10426 10427 10428 10429 10430 10431 10432 10640 Mon 08:30 E125 E125 E125 E125 E125 E125 E125 E125 E125 Tue 08:30 B303 Tue 10:00 A301 Tue 14:30 D208 Tue 16:00 B101 Thu 08:30 B203 Fri 08:30 B302 Fri 10:00 B302 Fri 13:00 B102 Sat 11:30 B303"},{"location":"course/2026_spring/","title":"Spring 2026","text":"<p>Important</p> <p>The content is subject to change. Please consistently check the course page on Blackboard and the ADA University Academic Calendar for modifications. The last day of the add/drop period, holidays, and similar dates are noted in the calendar.</p> Week Topic Learning Outcomes Assessment / Notes 1 Course Overview / Syllabus / Technology &amp; Engineering Describe the course structure and ground rules as defined in the syllabus. Define and describe the challenges of technology and engineering. \u2014 2 Data, Information, Knowledge / Statistics / Systems / Information Systems Differentiate between data and information. Describe the role of statistics. Define a system, its components, and interactions. Describe information systems and components of computer-based IS. \u2014 3 Digital vs Analog / Data Units / Number Systems / Binary Arithmetic / Two\u2019s Complement / Transistors / Boolean Logic Compare digital and analog systems. Define bit, byte, and data units. Convert numbers between decimal, binary, and hexadecimal. Explain binary addition, subtraction, and two\u2019s complement. Describe Boolean logic, truth tables, transistors, and logic gates. Project 1 out [Team]: Constructing basic logic gates 4 Hardware: CPU, GPU, Memory, I/O, Storage Explain von Neumann architecture. Describe CPU components and operation. Explain GPU purpose. Differentiate memory types (ROM, RAM, cache), DRAM vs SRAM, and secondary storage technologies. Homework 1 out: JavaScript programming 5 Algorithms I: Name Binding, Selection, Repetition Define algorithms. Explain basic algorithmic actions: name binding, selection, repetition. Project 2 out [Team]: Hour of Code 6 Algorithms II: Modularization, Recursion / Dijkstra\u2019s Algorithm Explain modularization and recursion. Describe Dijkstra\u2019s shortest path algorithm. \u2014 7 Software / Software Engineering / Programming Languages / Compilers &amp; Interpreters / Operating Systems Define software and software engineering. Explain best practices in software development. Differentiate syntax vs semantics. Explain programming paradigms and language evolution. Compare compiler and interpreter. Describe OS roles and activities. Midterm Exam: March 10 (Tuesday) 8 Telecommunications / Computer Networks / OSI Model / Network Topologies Describe telecommunication models and media. Explain network classifications and characteristics. Describe computer networks and OSI layers. Compare network types and topologies. Project 3 out [Team]: Programming LEGO robots 9 Internet / IP Suite / Addressing / DNS / Routing / Intranet &amp; Extranet / Distributed Systems Describe Internet structure. Explain IP suite, IP and MAC addresses. Describe DNS and routing. Define intranet, extranet, and distributed systems. \u2014 10 World Wide Web / Markup Languages / Cloud Computing / System Architectures Describe WWW structure and HTTP. Explain URLs. Describe markup languages. Explain cloud computing models and centralized vs decentralized systems. Homework 2 out: SQL 11 Databases / Data Models / Indexing / Normalization / Big Data Explain database approach and advantages. Describe ER modeling and indexing. Explain relational model and normalization. Define Big Data and knowledge discovery. \u2014 12 DBMS / SQL / NoSQL / Transactions / ACID / CAP Explain DBMS types and purposes. Describe data manipulation operations. Explain SQL commands. Contrast SQL and NoSQL. Explain transactions, ACID properties, and CAP theorem. Project 4 out [Individual]: Personal portfolio website 13 AI / ML / DL / Regression / Neural Networks / LLMs Define AI, ML, DL. Explain regression and classification. Describe supervised vs unsupervised learning. Explain linear regression and RMSE. Describe neural networks, LLMs, CSP backtracking, minimax, and reinforcement learning. \u2014 14 Systems Development / SDLC / Waterfall / Agile / Prototyping / Testing / UML Identify SDLC phases, participants, and outputs. Compare SDLC models. Explain DevOps and software testing. Describe UML diagrams (use case, class, sequence). \u2014 15 Engineering Ethics / Security / Privacy / Logic / Digital Hygiene Explain engineering ethics and social responsibility. Describe logical reasoning and fallacies. Outline ACM Code of Ethics. Explain digital hygiene and social issues of information systems. \u2014 \u2014 \u2014 \u2014 Final Exam: May 13 (Wednesday)"},{"location":"general/communication/","title":"Communication Rules","text":"<p>As the course is massive with hundreds of students, you are expected follow the communication rules throughout the semester. There are two options to communicate with instructors: 1) via virtual office hours on the course blackboard page, 2) via email.</p>"},{"location":"general/communication/#virtual-office-hours","title":"Virtual Office Hours","text":"<p>If it is a general-purpose question, an answer to which will be useful to other students, then a student must post it via virtual office hours on the blackboard discussions page. Virtual Office Hours is not a place for asking for specific help/guidance about your assignments (e.g. how to fix this or that error). Before posting, a student should check if:</p> <ul> <li>a similar question has already been asked by another student or not.</li> <li>the question has a meaningful title (for other students to navigate through).</li> <li>the answer can't easily be found with the help of google or other resources.</li> </ul>"},{"location":"general/communication/#emails","title":"Emails","text":"<p>If it is a personal (private) question, then a student should opt for an email. Emails should follow the provided template (see below) and have all the relevant emails in carbon copy (CC). For example, in case of writing an individual question to your instructor, you should put all co-instructor emails in CC. In case of writing an email regarding your team project, you should put all co-instructor AND all teammate emails in CC. </p> <p>Your emails should have a meaningful title (subject), introduction, body, and signature. The following is one possible template that you can use in your daily email conversations. You can automatically generate it for yourselves for ADA University. .</p>"},{"location":"general/communication/#extra-tips","title":"Extra tips","text":"<p>You can have different signatures for different courses or different emails. You can also use your signatures as your email templates. You can set up and use your signatures by clicking on the pen icon above your email (in Outlook, after clicking New Email), so that you won\u2019t have to copy-paste it every time.</p> <p>If your instructor has a Ph.D. degree, then refer to your instructor as Dr. or Professor. Otherwise, use Mr./Ms. (unless the person you are writing an email to has a title, such as M.D.). Use only surname immediately after.</p> <p>In case of requesting something, use the phrase \u201cThanks in advance\u201d at the end of your email in order to not write single-line and unnecessary \u201cThank you\u201d emails. Avoid excess emails and do not use Outlook as a chat box.</p> <p>Do not click REPLY ALL when your reply concerns only the sender. Use REPLY instead. Learn to make proper use of forwarding an email when necessary. See the official Microsoft instructions on the matter. You can also flag or pin important emails if you wish.</p>"},{"location":"general/resources/","title":"Policies &amp; Resources","text":"<p>ADA University has her university-wide policy documents. You are expected to carefully study at least the following short documents: Honor Code and Student Code of Conduct. You should also be aware of the Student Assessment Regulations. If you have strong reasons to disagree with the grade you got, you should follow the procedures written in Student Academic Grievance Policy. </p> <p>Students with special needs or with chronic health issues are strongly recommended to contact the University\u2019s Student Academic Support Services. ADA University provides upon request appropriate academic accommodations for qualified students with documented disabilities. Any student who feels (s)he may need accommodation based on the impact of a disability should notify the Office of Disability Services and Inclusive Education about their needs before the start of the academic term.<sup>1</sup></p> <p>Adjusting to student life, pursuing academic and personal goals can be emotionally stressful and challenging. Students are encouraged to make individual appointments with a Counselor to receive professional psychological support.</p> <p>Students are encouraged to consult with the Writing Center for checking their papers and assignments. Please visit the center or contact them by email. Reasonable accommodation is possible for students\u2019 religious beliefs, observations, and practices or for foreseeable conflicts because of athletic competition. Students with recurring or permanent internet and/or hardware problems are strongly recommended to contact the University\u2019s IT Service Desk and/or the Blackboard Administrator.</p> <ol> <li> <p>Contact information of different services is provided in the course Syllabus shared via Blackboard.\u00a0\u21a9</p> </li> </ol>"},{"location":"projects/01_project/","title":"Project 1","text":"<p>In this SITE 1101 team project, students explore how computer hardware works by physically building and testing basic digital logic. The project focuses on constructing NOT, AND, and OR logic gates from scratch using transistors, resistors, LEDs, and a breadboard in our beautiful lab room B012 managed by our great Lab Coordinator Nariman Vahabli. Bonus tasks extend learning by combining gates to create NAND and XOR logic through collaboration. Below are some of the finest videos by students.</p> Team 30: Suat Hamzali, Arif Badalov, Mahammad Sheykhov, Shakir Badalzada Fall 2025 Team 18: Kamal Badalzada, Aliyiyakbar Shirinli, Sonakhanim Dadashova, Zeynab Ajalova Fall 2025 Team 14: Hamayil Mammadova, Ilaha Mikayilova, Nihad Alakbarli, Shahin Guliyev Fall 2025 Team 81: Huseyn Huseynzada, Gulnar Feyzullayeva, Kamran Gafarli, Gulu Guseinov Fall 2025 Team 12: Gulshan Aliyeva, Aytaj Mirzazada, Narmin Bakirzada, Zeynal Ismayilzada Fall 2025 Team 37: Sami Karimli, Aytaj Aliyeva, Mahammad Mirzayev, Ali Aliyev Fall 2025 Team 39: Muhammad Muradov, Amil Isgandarov, Saleh Abbasov, Zaur Mirzaliyev Fall 2025"},{"location":"projects/02_project/","title":"Project 2","text":"<p>Info</p> <p>The page is under construction. Please check again later.</p>"},{"location":"projects/03_project/","title":"Project 3","text":"<p>Info</p> <p>The page is under construction. Please check again later.</p>"},{"location":"projects/04_project/","title":"Project 4","text":"<p>Info</p> <p>The page is under construction. Please check again later.</p>"}]}